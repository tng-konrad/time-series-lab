{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-01",
   "metadata": {},
   "source": [
    "# HALT: Hallucination Assessment via Log-probs as Time Series\n",
    "\n",
    "**Paper**: Shapiro, Taneja & Goel (2026) — arXiv:2602.02888\n",
    "\n",
    "## Overview\n",
    "\n",
    "HALT is a lightweight hallucination detector that treats the top-k token log-probabilities produced by an LLM as a **multivariate time series** and classifies each response as hallucinated or faithful using a compact **Bidirectional GRU**.\n",
    "\n",
    "### Key design decisions\n",
    "| Component | Detail |\n",
    "|---|---|\n",
    "| Input features | Top-20 log-probs + 5 engineered uncertainty statistics = **25-dim** vector per token |\n",
    "| Input projection | LayerNorm → 2-layer MLP → 128-dim |\n",
    "| Encoder | BiGRU, hidden=256, layers=5, dropout=0.4 → output 512-dim per step |\n",
    "| Pooling | **Top-q** (q=0.15): average the 15% of timesteps with largest ℓ₂ norm |\n",
    "| Classifier | Linear(512 → 1), BCEWithLogitsLoss |\n",
    "| Optimizer | Adam, lr=4.41e-4, weight_decay=2.34e-6 |\n",
    "\n",
    "### Tensor shape table\n",
    "```\n",
    "raw_logprobs  : (B, T, 20)\n",
    "eng_features  : (B, T, 5)   [entropy_overall, entropy_alts, avg_logprob, rank_proxy, dec_entropy_delta]\n",
    "x             : (B, T, 25)  concatenated input\n",
    "x_proj        : (B, T, 128) after MLP projection\n",
    "gru_out       : (B, T, 512) bidirectional hidden states\n",
    "pooled        : (B, 512)    top-q pooling\n",
    "logit         : (B, 1)      classifier output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Environment / installs ────────────────────────────────────────────────────\n",
    "# Uncomment if running in a fresh environment\n",
    "# !pip install torch torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Imports ───────────────────────────────────────────────────────────────────\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Config ────────────────────────────────────────────────────────────────────\n",
    "@dataclass\n",
    "class HALTConfig:\n",
    "    # Feature dimensions\n",
    "    top_k: int = 20                  # number of top log-probs per token\n",
    "    n_eng_features: int = 5          # engineered uncertainty features\n",
    "    input_dim: int = 25              # top_k + n_eng_features\n",
    "\n",
    "    # Projection MLP\n",
    "    proj_dim: int = 128\n",
    "\n",
    "    # BiGRU encoder\n",
    "    hidden_dim: int = 256            # per-direction hidden size\n",
    "    n_gru_layers: int = 5\n",
    "    gru_dropout: float = 0.4\n",
    "    bidirectional: bool = True\n",
    "\n",
    "    # Top-q pooling\n",
    "    top_q: float = 0.15              # fraction of timesteps to pool\n",
    "\n",
    "    # Classifier\n",
    "    out_norm: bool = False           # LayerNorm before linear (disabled in best setting)\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    lr: float = 4.41e-4\n",
    "    weight_decay: float = 2.34e-6\n",
    "    max_epochs: int = 100\n",
    "    early_stop_patience: int = 15\n",
    "    lr_scheduler_patience: int = 3\n",
    "    lr_scheduler_factor: float = 0.5\n",
    "    grad_clip_max_norm: float = 1.0\n",
    "\n",
    "    @property\n",
    "    def gru_output_dim(self) -> int:\n",
    "        \"\"\"BiGRU concatenates forward + backward: 256 * 2 = 512.\"\"\"\n",
    "        return self.hidden_dim * (2 if self.bidirectional else 1)\n",
    "\n",
    "\n",
    "cfg = HALTConfig()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-05",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Given the top-k log-probability matrix `logprobs` of shape `(T, k)` for a single response, we derive **5 scalar features per timestep**.\n",
    "\n",
    "| # | Name | Equation |\n",
    "|---|---|---|\n",
    "| 1 | `avg_logprob` | mean of top-k log-probs |\n",
    "| 2 | `rank_proxy` | rank of selected token within top-k window |\n",
    "| 3 | `entropy_overall` | H(p̃) over renormalised top-k |\n",
    "| 4 | `entropy_alts` | H(p̃_alts) over renormalised alternatives |\n",
    "| 5 | `dec_entropy_delta` | Δ of binary decision entropy between selected and best alternative |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Feature Extraction ────────────────────────────────────────────────────────\n",
    "\n",
    "def safe_entropy(probs: torch.Tensor, eps: float = 1e-9) -> torch.Tensor:\n",
    "    \"\"\"Shannon entropy of a probability vector. Shape: (...,) → scalar along last dim.\"\"\"\n",
    "    return -(probs * (probs + eps).log()).sum(dim=-1)\n",
    "\n",
    "\n",
    "def renorm_top_k(logprobs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Numerically stable softmax over top-k log-probs (Eq. 4).\n",
    "    logprobs: (T, k)  →  p_tilde: (T, k)\n",
    "    \"\"\"\n",
    "    m = logprobs.max(dim=-1, keepdim=True).values          # (T, 1)\n",
    "    exp_l = (logprobs - m).exp()                            # (T, k)\n",
    "    return exp_l / exp_l.sum(dim=-1, keepdim=True)          # (T, k)\n",
    "\n",
    "\n",
    "def extract_features(logprobs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extract 5 engineered uncertainty features for each token.\n",
    "\n",
    "    Args:\n",
    "        logprobs: (T, k)  — top-k log-probs; column 0 is the selected token.\n",
    "\n",
    "    Returns:\n",
    "        features: (T, 5)  — [avg_logprob, rank_proxy, entropy_overall,\n",
    "                                  entropy_alts, dec_entropy_delta]\n",
    "    \"\"\"\n",
    "    T, k = logprobs.shape\n",
    "\n",
    "    # ── 1. Average log-probability (Eq. 5) ───────────────────────────────────\n",
    "    avg_logprob = logprobs.mean(dim=-1, keepdim=True)              # (T, 1)\n",
    "\n",
    "    # ── 2. Rank proxy (Eq. 6) ────────────────────────────────────────────────\n",
    "    selected_lp = logprobs[:, 0:1]                                  # (T, 1)\n",
    "    alts_lp     = logprobs[:, 1:]                                   # (T, k-1)\n",
    "    rank_proxy  = 1.0 + (alts_lp > selected_lp).float().sum(dim=-1, keepdim=True)  # (T, 1)\n",
    "\n",
    "    # ── Renormalised distribution (Eq. 4) ───────────────────────────────────\n",
    "    p_tilde = renorm_top_k(logprobs)                                # (T, k)\n",
    "\n",
    "    # ── 3. Overall entropy (Eq. 7) ──────────────────────────────────────────\n",
    "    entropy_overall = safe_entropy(p_tilde).unsqueeze(-1)           # (T, 1)\n",
    "\n",
    "    # ── 4. Alternatives-only entropy (Eq. 8-9) ──────────────────────────────\n",
    "    p_alts       = p_tilde[:, 1:]                                   # (T, k-1)\n",
    "    p_alts_norm  = p_alts / (p_alts.sum(dim=-1, keepdim=True) + 1e-9)  # (T, k-1)\n",
    "    entropy_alts = safe_entropy(p_alts_norm).unsqueeze(-1)          # (T, 1)\n",
    "\n",
    "    # ── 5. Decision entropy delta (Eq. 10-13) ───────────────────────────────\n",
    "    best_alt_lp  = alts_lp.max(dim=-1).values                      # (T,)\n",
    "    # Binary probability of selected vs best alternative\n",
    "    denom        = (selected_lp.squeeze() + best_alt_lp).exp()      # avoid log-sum-exp issues\n",
    "    pc           = selected_lp.squeeze().exp() / (\n",
    "                       selected_lp.squeeze().exp() + best_alt_lp.exp() + 1e-9\n",
    "                   )                                                  # (T,)\n",
    "    h_dec        = -(pc * (pc + 1e-9).log()\n",
    "                     + (1 - pc) * (1 - pc + 1e-9).log())            # (T,)\n",
    "    # Temporal delta (Eq. 13); pad t=0 with 0\n",
    "    delta_h_dec  = torch.cat([\n",
    "        torch.zeros(1, device=logprobs.device),\n",
    "        h_dec[1:] - h_dec[:-1]\n",
    "    ]).unsqueeze(-1)                                                  # (T, 1)\n",
    "\n",
    "    # ── Concatenate all 5 features ───────────────────────────────────────────\n",
    "    features = torch.cat([\n",
    "        avg_logprob,       # (T, 1)\n",
    "        rank_proxy,        # (T, 1)\n",
    "        entropy_overall,   # (T, 1)\n",
    "        entropy_alts,      # (T, 1)\n",
    "        delta_h_dec        # (T, 1)\n",
    "    ], dim=-1)             # (T, 5)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def build_input_sequence(logprobs: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build the full enriched feature sequence l̃_{1:T} (Section 3.2).\n",
    "\n",
    "    Args:\n",
    "        logprobs: (T, k)  — raw top-k log-probabilities\n",
    "\n",
    "    Returns:\n",
    "        x_tilde: (T, k+5) — [engineered_features ∥ raw_logprobs]\n",
    "    \"\"\"\n",
    "    eng = extract_features(logprobs)                # (T, 5)\n",
    "    return torch.cat([eng, logprobs], dim=-1)        # (T, 25)\n",
    "\n",
    "\n",
    "# ── Quick shape test ─────────────────────────────────────────────────────────\n",
    "dummy_logprobs = -torch.rand(37, 20) * 10           # T=37 tokens, k=20\n",
    "dummy_x        = build_input_sequence(dummy_logprobs)\n",
    "print(f\"logprobs shape : {dummy_logprobs.shape}\")   # (37, 20)\n",
    "print(f\"x_tilde shape  : {dummy_x.shape}\")          # (37, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-07",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The `HALTDataset` accepts a list of `(logprobs_tensor, label)` pairs where:\n",
    "- `logprobs_tensor` : `(T_i, 20)` — variable-length top-20 log-prob sequence for response *i*\n",
    "- `label` : `int` — 1 = hallucinated, 0 = faithful\n",
    "\n",
    "The collate function pads sequences to the batch-maximum length and returns a boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Dataset & DataLoader ──────────────────────────────────────────────────────\n",
    "\n",
    "class HALTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each sample is a tuple:\n",
    "        logprobs : Tensor (T_i, top_k)  — raw log-probabilities\n",
    "        label    : int                  — 1 hallucinated, 0 faithful\n",
    "    \"\"\"\n",
    "    def __init__(self, samples: List[Tuple[torch.Tensor, int]]):\n",
    "        self.samples = samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        logprobs, label = self.samples[idx]         # (T_i, k), int\n",
    "        x_tilde = build_input_sequence(logprobs)    # (T_i, 25)\n",
    "        length  = x_tilde.shape[0]\n",
    "        return x_tilde, torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "\n",
    "def halt_collate_fn(\n",
    "    batch: List[Tuple[torch.Tensor, torch.Tensor, int]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Pads variable-length sequences to batch-max length.\n",
    "\n",
    "    Returns:\n",
    "        x_padded : (B, T_max, 25)  padded feature sequences\n",
    "        labels   : (B,)            binary labels\n",
    "        lengths  : (B,)            original sequence lengths (for packing)\n",
    "        mask     : (B, T_max)      True at valid (non-padded) positions\n",
    "    \"\"\"\n",
    "    x_list, labels, lengths = zip(*batch)\n",
    "    lengths  = torch.tensor(lengths, dtype=torch.long)\n",
    "    labels   = torch.stack(labels)                             # (B,)\n",
    "    x_padded = pad_sequence(x_list, batch_first=True)          # (B, T_max, 25)\n",
    "\n",
    "    B, T_max, _ = x_padded.shape\n",
    "    mask = torch.arange(T_max).unsqueeze(0) < lengths.unsqueeze(1)  # (B, T_max)\n",
    "\n",
    "    return x_padded, labels, lengths, mask\n",
    "\n",
    "\n",
    "def make_dataloader(samples, batch_size: int, shuffle: bool) -> DataLoader:\n",
    "    ds = HALTDataset(samples)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=halt_collate_fn,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# ── Synthetic data for demonstration ─────────────────────────────────────────\n",
    "def make_synthetic_dataset(n: int = 1000, top_k: int = 20,\n",
    "                            min_len: int = 10, max_len: int = 150) -> List[Tuple[torch.Tensor, int]]:\n",
    "    \"\"\"\n",
    "    Creates synthetic log-prob sequences with a simple hallucination signal:\n",
    "    hallucinated responses have injected entropy spikes at random positions.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        T     = random.randint(min_len, max_len)\n",
    "        label = random.randint(0, 1)\n",
    "        # Base: top token gets high prob, alternatives decay\n",
    "        lp = torch.zeros(T, top_k)\n",
    "        for i in range(top_k):\n",
    "            lp[:, i] = -float(i) * (0.5 + torch.rand(T) * 0.5)\n",
    "        if label == 1:\n",
    "            # Inject flatter distributions at random spike positions\n",
    "            n_spikes = random.randint(1, max(1, T // 10))\n",
    "            spike_pos = random.sample(range(T), min(n_spikes, T))\n",
    "            for pos in spike_pos:\n",
    "                lp[pos] = -torch.rand(top_k) * 2.0     # flatter = more uniform\n",
    "        samples.append((lp, label))\n",
    "    return samples\n",
    "\n",
    "\n",
    "train_samples = make_synthetic_dataset(n=800)\n",
    "val_samples   = make_synthetic_dataset(n=100)\n",
    "test_samples  = make_synthetic_dataset(n=100)\n",
    "\n",
    "train_loader = make_dataloader(train_samples, cfg.batch_size, shuffle=True)\n",
    "val_loader   = make_dataloader(val_samples,   cfg.batch_size, shuffle=False)\n",
    "test_loader  = make_dataloader(test_samples,  cfg.batch_size, shuffle=False)\n",
    "\n",
    "# Shape sanity check\n",
    "xb, lb, lensb, maskb = next(iter(train_loader))\n",
    "print(f\"x_padded : {xb.shape}\")\n",
    "print(f\"labels   : {lb.shape}\")\n",
    "print(f\"lengths  : {lensb.shape}\")\n",
    "print(f\"mask     : {maskb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-09",
   "metadata": {},
   "source": [
    "## HALT Model\n",
    "\n",
    "Architecture (Figure 1 + Appendix B):\n",
    "```\n",
    "x̃_{1:T}  (B, T, 25)\n",
    "    │\n",
    "    ▼  LayerNorm + 2-layer MLP  (GELU)\n",
    "x_proj   (B, T, 128)\n",
    "    │\n",
    "    ▼  pack_padded_sequence\n",
    "    ▼  BiGRU  (hidden=256, layers=5, dropout=0.4)\n",
    "    ▼  pad_packed_sequence\n",
    "H        (B, T, 512)\n",
    "    │\n",
    "    ▼  Top-q pooling  (q=0.15, score=||h_t||₂)\n",
    "pooled   (B, 512)\n",
    "    │\n",
    "    ▼  [optional LayerNorm]\n",
    "    ▼  Linear(512 → 1)\n",
    "logit    (B, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Input Projection (LayerNorm + MLP) ───────────────────────────────────────\n",
    "\n",
    "class InputProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects enriched feature vectors from input_dim → proj_dim.\n",
    "    Applied independently to each timestep.\n",
    "\n",
    "    Input  : (B, T, input_dim=25)\n",
    "    Output : (B, T, proj_dim=128)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, proj_dim: int):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.mlp  = nn.Sequential(\n",
    "            nn.Linear(input_dim, proj_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(proj_dim, proj_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, T, input_dim)\n",
    "        x = self.norm(x)        # (B, T, input_dim)\n",
    "        x = self.mlp(x)         # (B, T, proj_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ── Top-q Pooling ─────────────────────────────────────────────────────────────\n",
    "\n",
    "class TopQPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Averages the top-q fraction of timesteps scored by their ℓ₂ norm.\n",
    "    Padded positions are excluded via a boolean mask.\n",
    "\n",
    "    Input  : H    (B, T, D),  mask (B, T)  [True = valid]\n",
    "    Output : pooled (B, D)\n",
    "    \"\"\"\n",
    "    def __init__(self, q: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, H: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # H    : (B, T, D)\n",
    "        # mask : (B, T)  — True at valid positions\n",
    "        B, T, D = H.shape\n",
    "\n",
    "        scores = H.norm(dim=-1)                         # (B, T)  ℓ₂ norm\n",
    "        # Zero out padding\n",
    "        scores = scores.masked_fill(~mask, float(\"-inf\"))  # (B, T)\n",
    "\n",
    "        # Compute K = ceil(q * valid_length) per sequence\n",
    "        valid_lengths = mask.sum(dim=1).float()          # (B,)\n",
    "        K_per_seq     = (self.q * valid_lengths).ceil().clamp(min=1).long()  # (B,)\n",
    "        K             = int(K_per_seq.max().item())       # scalar, batch max K\n",
    "\n",
    "        # Take global top-K indices (simpler; matches paper's description)\n",
    "        _, top_idx = scores.topk(K, dim=1)               # (B, K)\n",
    "\n",
    "        # Gather hidden states at top-K timesteps\n",
    "        idx_exp    = top_idx.unsqueeze(-1).expand(-1, -1, D)  # (B, K, D)\n",
    "        top_states = H.gather(1, idx_exp)                # (B, K, D)\n",
    "\n",
    "        # Average\n",
    "        pooled = top_states.mean(dim=1)                  # (B, D)\n",
    "        return pooled\n",
    "\n",
    "\n",
    "# ── Full HALT Model ───────────────────────────────────────────────────────────\n",
    "\n",
    "class HALT(nn.Module):\n",
    "    \"\"\"\n",
    "    HALT: Hallucination Assessment via Log-probs as Time series.\n",
    "\n",
    "    Forward pass:\n",
    "        x       : (B, T, 25)  padded enriched feature sequences\n",
    "        lengths : (B,)        actual sequence lengths\n",
    "        mask    : (B, T)      True at valid positions\n",
    "\n",
    "    Returns:\n",
    "        logit   : (B,)        unnormalised hallucination score\n",
    "    \"\"\"\n",
    "    def __init__(self, config: HALTConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Input projection\n",
    "        self.projection = InputProjection(\n",
    "            input_dim=config.input_dim,\n",
    "            proj_dim=config.proj_dim,\n",
    "        )\n",
    "\n",
    "        # Bidirectional GRU\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=config.proj_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.n_gru_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=config.bidirectional,\n",
    "            dropout=config.gru_dropout if config.n_gru_layers > 1 else 0.0,\n",
    "        )\n",
    "\n",
    "        # Top-q pooling\n",
    "        self.pool = TopQPooling(q=config.top_q)\n",
    "\n",
    "        # Optional output LayerNorm\n",
    "        self.out_norm = (\n",
    "            nn.LayerNorm(config.gru_output_dim) if config.out_norm else nn.Identity()\n",
    "        )\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(config.gru_output_dim, 1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,           # (B, T, 25)\n",
    "        lengths: torch.Tensor,     # (B,)\n",
    "        mask: torch.Tensor         # (B, T)  bool\n",
    "    ) -> torch.Tensor:             # (B,)\n",
    "\n",
    "        # ── 1. Project input ─────────────────────────────────────────────────\n",
    "        x_proj = self.projection(x)                    # (B, T, 128)\n",
    "\n",
    "        # ── 2. Pack → BiGRU → Unpack ─────────────────────────────────────────\n",
    "        packed      = pack_padded_sequence(\n",
    "            x_proj, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        gru_packed, _ = self.gru(packed)\n",
    "        gru_out, _    = pad_packed_sequence(gru_packed, batch_first=True)  # (B, T, 512)\n",
    "\n",
    "        # Pad to original T if pad_packed_sequence truncates\n",
    "        T_padded = x.shape[1]\n",
    "        if gru_out.shape[1] < T_padded:\n",
    "            pad_size = T_padded - gru_out.shape[1]\n",
    "            gru_out  = F.pad(gru_out, (0, 0, 0, pad_size))   # (B, T, 512)\n",
    "\n",
    "        # ── 3. Top-q pooling ─────────────────────────────────────────────────\n",
    "        pooled = self.pool(gru_out, mask)               # (B, 512)\n",
    "\n",
    "        # ── 4. Classifier ────────────────────────────────────────────────────\n",
    "        pooled = self.out_norm(pooled)                  # (B, 512)\n",
    "        logit  = self.classifier(pooled).squeeze(-1)   # (B,)\n",
    "        return logit\n",
    "\n",
    "\n",
    "# ── Instantiate and verify ────────────────────────────────────────────────────\n",
    "model = HALT(cfg).to(device)\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {n_params:,}  ({n_params/1e6:.2f}M)\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Forward pass shape verification ──────────────────────────────────────────\n",
    "with torch.no_grad():\n",
    "    xb_d    = xb.to(device)\n",
    "    lensb_d = lensb.to(device)\n",
    "    maskb_d = maskb.to(device)\n",
    "    logit_test = model(xb_d, lensb_d, maskb_d)\n",
    "    print(f\"Input  x     : {xb_d.shape}\")\n",
    "    print(f\"Output logit : {logit_test.shape}\")\n",
    "    assert logit_test.shape == (xb_d.shape[0],), \"Shape mismatch!\"\n",
    "    print(\"✓ Forward pass OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Training Utilities\n",
    "\n",
    "Loss, optimiser, LR scheduler, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Loss & Optimiser ──────────────────────────────────────────────────────────\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",          # maximise macro-F1\n",
    "    factor=cfg.lr_scheduler_factor,\n",
    "    patience=cfg.lr_scheduler_patience,\n",
    ")\n",
    "\n",
    "print(\"Criterion  :\", criterion)\n",
    "print(\"Optimiser  :\", optimizer)\n",
    "print(\"Scheduler  :\", scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Metrics ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "def macro_f1(\n",
    "    logits: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    threshold: float = 0.5\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Macro-averaged F1 across two classes (halluicnated / faithful).\n",
    "    Primary metric in the paper.\n",
    "    \"\"\"\n",
    "    preds = (logits.sigmoid() >= threshold).long()\n",
    "    y     = labels.long()\n",
    "\n",
    "    f1_scores = []\n",
    "    for cls in [0, 1]:\n",
    "        tp = ((preds == cls) & (y == cls)).sum().float()\n",
    "        fp = ((preds == cls) & (y != cls)).sum().float()\n",
    "        fn = ((preds != cls) & (y == cls)).sum().float()\n",
    "        prec   = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1     = 2 * prec * recall / (prec + recall + 1e-9)\n",
    "        f1_scores.append(f1.item())\n",
    "\n",
    "    return float(np.mean(f1_scores))\n",
    "\n",
    "\n",
    "def accuracy(logits: torch.Tensor, labels: torch.Tensor, threshold: float = 0.5) -> float:\n",
    "    preds = (logits.sigmoid() >= threshold).long()\n",
    "    return (preds == labels.long()).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Train Step ────────────────────────────────────────────────────────────────\n",
    "\n",
    "def train_step(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: nn.Module,\n",
    "    grad_clip: float,\n",
    "    dev: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    One epoch of training.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss : float\n",
    "        macro_f1 : float\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    for x, labels, lengths, mask in loader:\n",
    "        x, labels = x.to(dev), labels.to(dev)\n",
    "        lengths, mask = lengths.to(dev), mask.to(dev)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, lengths, mask)               # (B,)\n",
    "        loss   = criterion(logits, labels)             # scalar\n",
    "        loss.backward()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    avg_loss   = total_loss / len(all_labels)\n",
    "    mf1        = macro_f1(all_logits, all_labels)\n",
    "    return avg_loss, mf1\n",
    "\n",
    "\n",
    "# ── Eval Step ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_step(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    dev: torch.device,\n",
    ") -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluation on a given DataLoader.\n",
    "\n",
    "    Returns:\n",
    "        avg_loss : float\n",
    "        macro_f1 : float\n",
    "        acc      : float\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits, all_labels = [], []\n",
    "\n",
    "    for x, labels, lengths, mask in loader:\n",
    "        x, labels = x.to(dev), labels.to(dev)\n",
    "        lengths, mask = lengths.to(dev), mask.to(dev)\n",
    "\n",
    "        logits = model(x, lengths, mask)               # (B,)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    avg_loss   = total_loss / len(all_labels)\n",
    "    mf1        = macro_f1(all_logits, all_labels)\n",
    "    acc        = accuracy(all_logits, all_labels)\n",
    "    return avg_loss, mf1, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "With early stopping (patience = 15) and ReduceLROnPlateau (patience = 3), both monitoring **macro-F1** on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Main Training Loop ────────────────────────────────────────────────────────\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    criterion: nn.Module,\n",
    "    config: HALTConfig,\n",
    "    dev: torch.device,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Full training run with early stopping.\n",
    "\n",
    "    Returns:\n",
    "        history : dict with train/val loss and F1 per epoch\n",
    "    \"\"\"\n",
    "    best_val_f1     = -1.0\n",
    "    epochs_no_imprv = 0\n",
    "    best_state      = None\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [], \"train_f1\": [],\n",
    "        \"val_loss\": [],   \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, config.max_epochs + 1):\n",
    "        tr_loss, tr_f1 = train_step(\n",
    "            model, train_loader, optimizer, criterion, config.grad_clip_max_norm, dev\n",
    "        )\n",
    "        va_loss, va_f1, va_acc = eval_step(model, val_loader, criterion, dev)\n",
    "\n",
    "        scheduler.step(va_f1)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_f1\"].append(tr_f1)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"val_f1\"].append(va_f1)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{config.max_epochs} | \"\n",
    "            f\"Train loss={tr_loss:.4f} F1={tr_f1:.4f} | \"\n",
    "            f\"Val loss={va_loss:.4f} F1={va_f1:.4f} Acc={va_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping\n",
    "        if va_f1 > best_val_f1:\n",
    "            best_val_f1     = va_f1\n",
    "            epochs_no_imprv = 0\n",
    "            best_state      = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_no_imprv += 1\n",
    "            if epochs_no_imprv >= config.early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best val macro-F1={best_val_f1:.4f})\")\n",
    "                break\n",
    "\n",
    "    # Restore best checkpoint\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"\\nRestored best model  (val macro-F1={best_val_f1:.4f})\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Run Training ──────────────────────────────────────────────────────────────\n",
    "history = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    criterion=criterion,\n",
    "    config=cfg,\n",
    "    dev=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Test Evaluation ───────────────────────────────────────────────────────────\n",
    "test_loss, test_f1, test_acc = eval_step(model, test_loader, criterion, device)\n",
    "print(f\"\\n=== Test Results ===\")\n",
    "print(f\"  Loss      : {test_loss:.4f}\")\n",
    "print(f\"  Macro-F1  : {test_f1:.4f}\")\n",
    "print(f\"  Accuracy  : {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Training Curves ───────────────────────────────────────────────────────────\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].plot(epochs, history[\"train_loss\"], label=\"Train\")\n",
    "    axes[0].plot(epochs, history[\"val_loss\"],   label=\"Val\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"BCE Loss\")\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(epochs, history[\"train_f1\"], label=\"Train\")\n",
    "    axes[1].plot(epochs, history[\"val_f1\"],   label=\"Val\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"Macro-F1\")\n",
    "    axes[1].set_title(\"Macro-F1\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.suptitle(\"HALT Training Curves\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"halt_training_curves.png\", dpi=120)\n",
    "    plt.show()\n",
    "    print(\"Saved: halt_training_curves.png\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib not installed — skipping plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Inference on a single response ───────────────────────────────────────────\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_hallucination(\n",
    "    logprobs: torch.Tensor,\n",
    "    model: nn.Module,\n",
    "    dev: torch.device,\n",
    "    threshold: float = 0.5,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Predict hallucination probability for a single response.\n",
    "\n",
    "    Args:\n",
    "        logprobs : (T, 20)  — raw top-20 log-probs from the LLM\n",
    "        model    : trained HALT model\n",
    "        dev      : device\n",
    "        threshold: decision threshold\n",
    "\n",
    "    Returns:\n",
    "        dict with probability and binary prediction\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    x_tilde  = build_input_sequence(logprobs)           # (T, 25)\n",
    "    x_batch  = x_tilde.unsqueeze(0).to(dev)             # (1, T, 25)\n",
    "    lengths  = torch.tensor([x_tilde.shape[0]]).to(dev) # (1,)\n",
    "    mask     = torch.ones(1, x_tilde.shape[0], dtype=torch.bool, device=dev)  # (1, T)\n",
    "\n",
    "    logit = model(x_batch, lengths, mask)               # (1,)\n",
    "    prob  = logit.sigmoid().item()\n",
    "\n",
    "    return {\n",
    "        \"hallucination_probability\": prob,\n",
    "        \"is_hallucinated\": prob >= threshold,\n",
    "        \"threshold\": threshold,\n",
    "    }\n",
    "\n",
    "\n",
    "# Demo: predict on a single synthetic sample\n",
    "sample_logprobs, sample_label = test_samples[0]\n",
    "result = predict_hallucination(sample_logprobs, model, device)\n",
    "\n",
    "print(f\"True label              : {'hallucinated' if sample_label == 1 else 'faithful'}\")\n",
    "print(f\"Hallucination probability: {result['hallucination_probability']:.4f}\")\n",
    "print(f\"Predicted               : {'hallucinated' if result['is_hallucinated'] else 'faithful'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model Checkpoint Save/Load ────────────────────────────────────────────────\n",
    "\n",
    "checkpoint_path = \"halt_checkpoint.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\"    : model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"config\"              : cfg,\n",
    "    \"history\"             : history,\n",
    "    \"test_macro_f1\"       : test_f1,\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "# Reload demo\n",
    "ckpt        = torch.load(checkpoint_path, map_location=device)\n",
    "model_reloaded = HALT(ckpt[\"config\"]).to(device)\n",
    "model_reloaded.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "print(\"Checkpoint loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Component | Implementation |\n",
    "|---|---|\n",
    "| **Feature extractor** | `extract_features()` — 5 uncertainty signals from top-k log-probs |\n",
    "| **Input builder** | `build_input_sequence()` — (T, 25) per response |\n",
    "| **Dataset** | `HALTDataset` + `halt_collate_fn` — handles variable lengths |\n",
    "| **Projection** | `InputProjection` — LayerNorm + 2-layer GELU MLP (25→128) |\n",
    "| **Encoder** | BiGRU (hidden=256, layers=5, dropout=0.4) → (B,T,512) |\n",
    "| **Pooling** | `TopQPooling` (q=0.15, ℓ₂-score) → (B,512) |\n",
    "| **Classifier** | `Linear(512→1)` + BCEWithLogitsLoss |\n",
    "| **Optimiser** | Adam, lr=4.41e-4, wd=2.34e-6 |\n",
    "| **Scheduler** | ReduceLROnPlateau (factor=0.5, patience=3, mode=max) |\n",
    "| **Early stopping** | patience=15 on val macro-F1 |\n",
    "| **Metric** | Macro-F1 (primary), accuracy (secondary) |\n",
    "\n",
    "To use with a **real LLM API**, replace `make_synthetic_dataset` with a function that:\n",
    "1. Sends prompts to the LLM (e.g., via OpenAI or vLLM)\n",
    "2. Extracts `logprobs` (shape `(T, 20)`) from the API response metadata\n",
    "3. Pairs each response with a hallucination label from your annotation pipeline\n",
    "\n",
    "HALT then trains a **model-specific** detector for that LLM's calibration bias."
   ]
  }
 ]
}
