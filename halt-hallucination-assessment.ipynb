{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2daaab6f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0e23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Third-party library imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd38e7f",
   "metadata": {},
   "source": [
    "The code imports standard library modules: `math` for mathematical operations, `random` for generating random numbers, `dataclass` for defining data classes, and typing modules (`Dict`, `List`, `Optional`, `Tuple`) for type hints.\n",
    "\n",
    "It imports third-party libraries: `matplotlib.pyplot` for plotting graphs, `numpy` for numerical computations, `sklearn.manifold.TSNE` for dimensionality reduction using t-SNE, `sklearn.metrics.f1_score` and `roc_auc_score` for evaluating classification models, and `sklearn.preprocessing.minmax_scale` for scaling features to a range.\n",
    "\n",
    "It imports PyTorch components: `torch` as the main package, `torch.nn` for neural network layers and functions, utilities for handling variable-length sequences (`pack_padded_sequence`, `pad_packed_sequence`, `pad_sequence`) from `torch.nn.utils.rnn`, and data loading tools (`DataLoader`, `Dataset`, `random_split`) from `torch.utils.data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608496f9",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd223e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "class CFG:\n",
    "    img_dim1 = 20\n",
    "    img_dim2 = 10\n",
    "    SEED = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# display style \n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams[\"figure.figsize\"] = (CFG.img_dim1, CFG.img_dim2)\n",
    "\n",
    "print(f\"Using device: {CFG.device}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# fix randomness (insofar as possible ;-)\n",
    "SEED = 42\n",
    "random.seed(CFG.SEED)\n",
    "np.random.seed(CFG.SEED)\n",
    "torch.manual_seed(CFG.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dccf491",
   "metadata": {},
   "source": [
    "The code defines configuration constants and sets up reproducible random number generation for a machine learning project.\n",
    "\n",
    "A class named `CFG` is created with four attributes:  \n",
    "- `img_dim1` set to 20, representing the first dimension of images (likely width),  \n",
    "- `img_dim2` set to 10, representing the second dimension (likely height),  \n",
    "- `SEED` set to 42 for reproducibility,  \n",
    "- `device` assigned `\"cuda\"` if CUDA is available (indicating GPU support), otherwise `\"cpu\"`.  \n",
    "\n",
    "Matplotlib's display settings are then configured: the style is set to `seaborn-v0_8`, and the figure size is updated using values from `CFG`.  \n",
    "\n",
    "The script prints which computing device it will use and the installed PyTorch version.  \n",
    "\n",
    "Random number generation is seeded consistently across Python's built-in `random` module, NumPy, and PyTorch (both CPU and GPU when CUDA is available), using the value 42 to ensure reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db9d68e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c65bd",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfe71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HALTConfig:\n",
    "    # Feature dimensions (Sec 3.2 + Fig.1)\n",
    "    top_k: int = 20\n",
    "    n_stat_features: int = 5  # [avg_logp, rank_proxy, h_overall, h_alts, delta_h_dec]\n",
    "    input_dim: int = 25       # top_k + n_stat_features\n",
    "\n",
    "    # Projection MLP (Sec 3.2, Appendix B)\n",
    "    proj_dim: int = 128\n",
    "\n",
    "    # Bidirectional GRU (Appendix B)\n",
    "    hidden_dim: int = 256\n",
    "    num_layers: int = 5\n",
    "    dropout: float = 0.4\n",
    "\n",
    "    # Top-q salient pooling (Eq Sec 3.2)\n",
    "    top_q: float = 0.15\n",
    "\n",
    "    # Training (Appendix B)\n",
    "    batch_size: int = 512\n",
    "    max_epochs: int = 5 # 100\n",
    "    lr: float = 4.41e-4\n",
    "    weight_decay: float = 2.34e-6\n",
    "    lr_patience: int = 3\n",
    "    lr_factor: float = 0.5\n",
    "    early_stop_patience: int = 15\n",
    "    max_grad_norm: float = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eed8b12",
   "metadata": {},
   "source": [
    "The `HALTConfig` class defines hyperparameters and architectural settings for a HALT model.  \n",
    "\n",
    "Feature dimensions are configured as follows:  \n",
    "- `top_k` is set to 20, indicating the number of top items considered for features.  \n",
    "- `n_stat_features` is set to 5, representing five additional statistical features: average log-probability, rank proxy, overall entropy (`h_overall`), alternative entropy (`h_alts`), and decrease in entropy (`delta_h_dec`).  \n",
    "- `input_dim` is computed as the sum of `top_k` and `n_stat_features`, resulting in 25.  \n",
    "\n",
    "A projection MLP has `proj_dim` set to 128, mapping input features into a shared embedding space.  \n",
    "\n",
    "A bidirectional GRU is configured with `hidden_dim` of 256, `num_layers` of 5, and `dropout` of 0.4 for regularization.  \n",
    "\n",
    "The `top_q` parameter is set to 0.15, indicating that the top 15% of salient elements are selected during pooling as described in Section 3.2.  \n",
    "\n",
    "Training settings include:  \n",
    "- `batch_size` of 512,  \n",
    "- `max_epochs` set to 5 (for fast iteration purpose - and synthetic data; obviously has to be higher for real dataset),  \n",
    "- `lr` (learning rate) at $4.41 \\times 10^{-4}$,  \n",
    "- `weight_decay` (L2 regularization coefficient) at $2.34 \\times 10^{-6}$,  \n",
    "- `lr_patience` of 3 epochs before reducing the learning rate,  \n",
    "- `lr_factor` of 0.5 to scale down the learning rate when plateauing,  \n",
    "- `early_stop_patience` of 15 epochs before halting training if no improvement occurs,  \n",
    "- `max_grad_norm` of 1.0 for gradient clipping to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fcb17",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    " (Sec 3.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0450be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPS = 1e-9\n",
    "\n",
    "def compute_engineered_features(logprobs_k: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    T, K = logprobs_k.shape\n",
    "\n",
    "    # Stable truncated distribution over top-k (Eq 4)\n",
    "    mt = logprobs_k.max(dim=1, keepdim=True).values\n",
    "    exp_s = torch.exp(logprobs_k - mt)\n",
    "    p_tilde = exp_s / (exp_s.sum(dim=1, keepdim=True) + EPS)\n",
    "\n",
    "    # 1. AvgLogP (Eq 5)\n",
    "    avg_logp = logprobs_k.mean(dim=1)  # [T]\n",
    "\n",
    "    # 2. RankProxy (Eq 6)\n",
    "    sel = logprobs_k[:, :1]            # [T, 1]\n",
    "    rank_proxy = 1.0 + (logprobs_k[:, 1:] > sel).float().sum(dim=1)  # [T]\n",
    "\n",
    "    # 3. Overall entropy over truncated distribution (Eq 7)\n",
    "    h_overall = -(p_tilde * torch.log(p_tilde + EPS)).sum(dim=1)  # [T]\n",
    "\n",
    "    # 4. Alternatives-only entropy (Eq 8‚Äì9)\n",
    "    p_alts = p_tilde[:, 1:]                            # [T, K-1]\n",
    "    p_alts_n = p_alts / (p_alts.sum(dim=1, keepdim=True) + EPS)\n",
    "    h_alts = -(p_alts_n * torch.log(p_alts_n + EPS)).sum(dim=1)   # [T]\n",
    "\n",
    "    # 5. Temporal delta of binary decision entropy (Eq 10‚Äì13)\n",
    "    best_alt = logprobs_k[:, 1:].max(dim=1).values     # [T]\n",
    "    sel_lp = logprobs_k[:, 0]                          # [T]\n",
    "\n",
    "    pc_num = torch.exp(sel_lp)\n",
    "    pc_den = torch.exp(sel_lp) + torch.exp(best_alt)   # [T]\n",
    "    pc = pc_num / (pc_den + EPS)\n",
    "    pc = pc.clamp(EPS, 1.0 - EPS)                      # avoid log(0)\n",
    "    h_dec = -(pc * torch.log(pc) + (1 - pc) * torch.log(1 - pc))  # [T]\n",
    "\n",
    "    delta_h_dec = h_dec.clone()\n",
    "    delta_h_dec[1:] = h_dec[1:] - h_dec[:-1]\n",
    "\n",
    "    return torch.stack([avg_logp, rank_proxy, h_overall, h_alts, delta_h_dec], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df76b06",
   "metadata": {},
   "source": [
    "The `compute_engineered_features` function computes five engineered features from a sequence of log-probabilities (`logprobs_k`) for each time step. The input tensor has shape `[T, K]`, where `T` is the number of time steps and `K` is the number of top candidates.\n",
    "\n",
    "First, a numerically stable truncated distribution is computed:  \n",
    "- The maximum log-probability per time step (`mt`) is subtracted before exponentiation to avoid overflow, yielding `exp_s`.  \n",
    "- Normalized probabilities (`p_tilde`) are obtained by dividing `exp_s` by its sum, with a small epsilon (`EPS = 1e-9`) added for numerical stability.\n",
    "\n",
    "Five features are then computed:\n",
    "\n",
    "1. **AvgLogP**: The mean of the original log-probabilities across the `K` candidates at each time step.\n",
    "\n",
    "2. **RankProxy**: A rank-based metric: for each time step, it compares the top log-probability (first element) to the remaining `K-1` values and counts how many are strictly greater, then adds 1.\n",
    "\n",
    "3. **Overall entropy (`h_overall`)**: Shannon entropy computed over the truncated distribution `p_tilde`, measuring uncertainty across all top-`K` candidates.\n",
    "\n",
    "4. **Alternatives-only entropy (`h_alts`)**: Shannon entropy computed over the distribution restricted to alternatives (excluding the top candidate). The probabilities of alternatives are renormalized before entropy computation.\n",
    "\n",
    "5. **Temporal delta of binary decision entropy (`delta_h_dec`)**:  \n",
    "   - The probability of selecting the top candidate versus the best alternative is computed via a binary softmax: `pc = exp(sel_lp) / (exp(sel_lp) + exp(best_alt))`, where `sel_lp` is the top candidate's log-probability and `best_alt` is the maximum among alternatives.  \n",
    "   - This `pc` is clamped to avoid log(0), and binary entropy `h_dec` is computed.  \n",
    "   - The temporal difference (`delta_h_dec`) is obtained by differencing consecutive entropy values.\n",
    "\n",
    "The function returns a tensor of shape `[T, 5]`, stacking the five features along the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40641efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_halt_input(logprobs_k: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Concatenate engineered stats with raw log-probs ‚Üí [T, 25].\n",
    "\n",
    "    Order: [stats (5) || raw_logprobs (20)] -> Fig. 1 / Sec 3.2.\n",
    "    \"\"\"\n",
    "    stats = compute_engineered_features(logprobs_k)     # [T, 5]\n",
    "    return torch.cat([stats, logprobs_k], dim=1)        # [T, 25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c08196b",
   "metadata": {},
   "source": [
    "The `build_halt_input` function constructs the model input by concatenating engineered statistical features with raw log-probabilities along the feature dimension.\n",
    "\n",
    "It takes `logprobs_k`, a tensor of shape `[T, K]` (where `K = 20`), representing log-probabilities over top-`K` candidates at each of `T` time steps.\n",
    "\n",
    "First, it computes the five engineered features (AvgLogP, RankProxy, Overall Entropy, Alternatives-Only Entropy, and Temporal Delta of Decision Entropy) by calling `compute_engineered_features`, resulting in a tensor of shape `[T, 5]`.\n",
    "\n",
    "Then, it concatenates these features with the original raw log-probabilities (`logprobs_k`) along dimension 1, yielding a final input tensor of shape `[T, 25]` ‚Äî matching the `input_dim` specified in `HALTConfig`.\n",
    "\n",
    "The concatenation order is explicitly: five engineered statistics followed by twenty raw log-probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95520bee",
   "metadata": {},
   "source": [
    "## Synthetic Dataset Generator \n",
    "\n",
    "(Sec 4.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _synthetic_logprobs(\n",
    "    seq_len: int,\n",
    "    top_k: int,\n",
    "    hallucinated: bool,\n",
    "    rng: np.random.Generator\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Synthesize \"realistic\" top-k log-prob matrix for hallucinated vs correct tokens.\n",
    "\n",
    "    Hallucinated: flatter distribution (small gaps, high noise)\n",
    "    Correct: peaked distribution (large gaps, low noise)\n",
    "\n",
    "    Returns:\n",
    "        [seq_len, top_k] float32 log-probs, sorted descending per row.\n",
    "    \"\"\"\n",
    "    result = np.zeros((seq_len, top_k), dtype=np.float32)\n",
    "    for t in range(seq_len):\n",
    "        if hallucinated:\n",
    "            base = rng.uniform(-4.0, -0.3)      # lower mean (more uncertainty)\n",
    "            gap  = rng.uniform(0.05, 0.3)       # narrow gaps (indistinguishable)\n",
    "        else:\n",
    "            base = rng.uniform(-1.5, -0.1)      # higher mean (confident)\n",
    "            gap  = rng.uniform(0.5, 2.0)        # large gaps (confident ranking)\n",
    "        # Alternatives: base - gap*(1..K-1) + small noise\n",
    "        alts = base - gap * np.arange(1, top_k)\n",
    "        alts += rng.uniform(-0.5, 0.5, top_k - 1)\n",
    "        result[t] = np.concatenate([[base], alts]).astype(np.float32)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9218e78",
   "metadata": {},
   "source": [
    "The `_synthetic_logprobs` function generates synthetic log-probability matrices for evaluation or training purposes, simulating two distinct scenarios: hallucinated and correct token generations.\n",
    "\n",
    "Inputs:\n",
    "- `seq_len`: number of time steps (sequence length),\n",
    "- `top_k`: number of top candidates per time step,\n",
    "- `hallucinated`: boolean flag indicating whether to generate hallucinated (uncertain) or correct (confident) log-probabilities,\n",
    "- `rng`: NumPy random number generator for reproducibility.\n",
    "\n",
    "For each time step `t` from 0 to `seq_len-1`, the function creates a row of `top_k` log-probabilities:\n",
    "\n",
    "- When `hallucinated=True`, it uses a lower baseline log-probability (`base` sampled uniformly from [-4.0, -0.3]) and small gaps (`gap` in [0.05, 0.3]), producing flatter distributions with indistinguishable candidates and higher uncertainty.\n",
    "- When `hallucinated=False`, it uses a higher baseline (`base` in [-1.5, -0.1]) and larger gaps (`gap` in [0.5, 2.0]), yielding peaked distributions where the top candidate is clearly favored.\n",
    "\n",
    "For alternatives (indices 1 to `top_k-1`), log-probabilities are constructed as `base - gap * i` for `i = 1..(K-1)`, with small uniform noise added (`[-0.5, 0.5]`). The top log-probability is set to `base`, and all values are concatenated and cast to float32.\n",
    "\n",
    "The output is a `[seq_len, top_k]` array with rows sorted in descending order (since `base` is the largest value by construction and gaps are positive), representing realistic synthetic log-probability distributions for both correct and hallucinated cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78035a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticLogProbDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic dataset: (feature_sequence, label) pairs.\n",
    "    One sample = one LLM response as top-k log-prob time series.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_samples: int = 4000,\n",
    "        top_k: int = 20,\n",
    "        min_len: int = 10,\n",
    "        max_len: int = 150,\n",
    "        hallucination_rate: float = 0.5,\n",
    "        seed: int = SEED,\n",
    "    ):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.samples: List[Tuple[torch.Tensor, int]] = []\n",
    "        for _ in range(n_samples):\n",
    "            label   = int(rng.random() < hallucination_rate)\n",
    "            seq_len = int(rng.integers(min_len, max_len + 1))\n",
    "            raw     = _synthetic_logprobs(seq_len, top_k, bool(label), rng)\n",
    "            x       = build_halt_input(torch.from_numpy(raw))\n",
    "            self.samples.append((x, label))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int, int]:\n",
    "        x, label = self.samples[idx]\n",
    "        return x, label, x.shape[0]   # tensor, label, length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2d730",
   "metadata": {},
   "source": [
    "The `SyntheticLogProbDataset` class implements a PyTorch Dataset for generating synthetic data used to train or evaluate the HALT model.\n",
    "\n",
    "Each sample corresponds to one simulated LLM response, represented as a time series of top-`K` log-probabilities with an associated binary label: 1 for hallucinated responses, 0 for correct ones.\n",
    "\n",
    "In the constructor:\n",
    "- A NumPy random number generator is initialized with the given seed for reproducibility.\n",
    "- For each of `n_samples` (default 4000) iterations:\n",
    "  - A binary label is sampled: hallucinated (`label=1`) with probability `hallucination_rate` (default 0.5), otherwise correct (`label=0`).\n",
    "  - A random sequence length is drawn uniformly from `[min_len, max_len]`, i.e., between 10 and 150 tokens.\n",
    "  - Synthetic log-probabilities are generated using `_synthetic_logprobs`, with the `hallucinated` flag set to the sampled label.\n",
    "  - The raw log-prob matrix (shape `[seq_len, top_k]`) is converted to a PyTorch tensor and passed through `build_halt_input` to form the full input features (shape `[seq_len, 25]`).\n",
    "  - The resulting tensor `x`, its label, and the sequence length are stored.\n",
    "\n",
    "The `__len__` method returns the total number of samples.\n",
    "\n",
    "The `__getitem__` method retrieves a single sample as a tuple:  \n",
    "- `x`: the full feature tensor of shape `[seq_len, 25]`,  \n",
    "- `label`: integer (0 or 1),  \n",
    "- `x.shape[0]`: the actual sequence length for that sample (needed e.g., for padding-aware batching)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d720e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    xs, labels, lengths = zip(*batch)\n",
    "    \n",
    "    # Convert list of tensors to padded tensor (requires_grad kept)\n",
    "    lengths_t = torch.tensor(lengths, dtype=torch.long)\n",
    "    idx       = lengths_t.argsort(descending=True)\n",
    "    \n",
    "    xs_sorted   = [xs[i] for i in idx]\n",
    "    lengths_s   = lengths_t[idx]\n",
    "    \n",
    "    # üîëÂÖ≥ÈîÆÔºöpad_sequence preserves requires_grad if inputs have it\n",
    "    padded = pad_sequence(xs_sorted, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # If needed, ensure grad flag is preserved:\n",
    "    # padded.retain_grad()  # optional, not needed here\n",
    "    \n",
    "    labels_t = torch.tensor([labels[i] for i in idx], dtype=torch.float)\n",
    "    \n",
    "    return padded, labels_t, lengths_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432d9fc",
   "metadata": {},
   "source": [
    "The `collate_fn` function processes a batch of samples from the `SyntheticLogProbDataset` to prepare it for model training or evaluation.\n",
    "\n",
    "Each input sample is a tuple `(x, label, length)`, where:\n",
    "- `x` is a 2D tensor of shape `[seq_len, 25]`,\n",
    "- `label` is an integer (0 or 1),\n",
    "- `length` is the original sequence length.\n",
    "\n",
    "The function groups inputs by unpacking the batch into three lists: `xs`, `labels`, and `lengths`.\n",
    "\n",
    "It then sorts the samples in descending order of sequence length using `argsort`, which is essential for efficient padded batching with most RNN implementations (longest sequences first). The indices `idx` indicate this sorting order.\n",
    "\n",
    "The sorted samples are reordered: `xs_sorted` contains tensors ordered by decreasing length, and `lengths_s` stores the corresponding sorted lengths.\n",
    "\n",
    "Padding is applied via `pad_sequence`, which pads shorter sequences to match the length of the longest sequence in the batch. Padding is done with zeros (`padding_value=0.0`), and `batch_first=True` ensures the output tensor has shape `[batch_size, max_seq_len, 25]`.\n",
    "\n",
    "Labels are extracted in the same sorted order and converted to a float tensor.\n",
    "\n",
    "The function returns a tuple:\n",
    "- `padded`: padded input tensor,\n",
    "- `labels_t`: label tensor (same order as `padded`),\n",
    "- `lengths_s`: sorted sequence lengths tensor (descending), used for unpadded length handling in models like GRUs.\n",
    "\n",
    "This collate function ensures variable-length sequences are handled efficiently while preserving gradient flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942898e8",
   "metadata": {},
   "source": [
    "## Model Blocks \n",
    "(Sec 3.2 + Appendix B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de212dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm ‚Üí Linear ‚Üí GELU ‚Üí Linear  (Sec 3.2)\n",
    "    [B,T,25] ‚Üí [B,T,128]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, proj_dim: int):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.fc1  = nn.Linear(input_dim, proj_dim)\n",
    "        self.act  = nn.GELU()\n",
    "        self.fc2  = nn.Linear(proj_dim, proj_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.fc2(self.act(self.fc1(self.norm(x))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3e87b",
   "metadata": {},
   "source": [
    "The `InputProjection` module performs a feedforward transformation on the input sequence to map features from dimensionality 25 to 128 (as defined in `HALTConfig`), following the architecture described in Section 3.2 and Appendix B.\n",
    "\n",
    "It consists of four sequential operations:\n",
    "\n",
    "1. **Layer Normalization (`nn.LayerNorm`)**: Applied across the feature dimension (last axis), normalizing each time step independently to stabilize training.\n",
    "2. **First Linear Layer (`fc1`)**: A linear transformation mapping the input features (dim=25) to a projection dimension of 128.\n",
    "3. **GELU Activation**: A smooth nonlinearity applied element-wise to introduce nonlinearity after the first linear layer.\n",
    "4. **Second Linear Layer (`fc2`)**: Maps the projected features back to the same dimension (128), completing a residual-free projection block.\n",
    "\n",
    "The forward pass takes an input tensor `x` of shape `[B, T, 25]` (batch √ó time steps √ó features) and returns a tensor of shape `[B, T, 128]`. LayerNorm is applied first to normalize inputs per time step before projection, which can improve convergence and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopQPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Average top-q salient timesteps (Sec 3.2).\n",
    "    [B,T,D], lengths ‚Üí [B,D]\n",
    "    \"\"\"\n",
    "    def __init__(self, q: float = 0.15):\n",
    "        super().__init__()\n",
    "        self.q = q\n",
    "\n",
    "    def forward(self, H: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, D = H.shape\n",
    "        mask   = torch.arange(T, device=H.device).unsqueeze(0) < lengths.unsqueeze(1)\n",
    "        scores = H.norm(dim=2).masked_fill(~mask, -1e9)  # L2 norm + padding mask\n",
    "        k_vals = (lengths.float() * self.q).ceil().long().clamp(min=1)\n",
    "\n",
    "        pooled = torch.zeros(B, D, device=H.device)\n",
    "        for b in range(B):\n",
    "            top_idx = scores[b].topk(k_vals[b].item()).indices\n",
    "            pooled[b] = H[b, top_idx].mean(dim=0)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391b949",
   "metadata": {},
   "source": [
    "The `TopQPooling` module implements top-`q` salient temporal pooling as described in Section 3.2.\n",
    "\n",
    "Given an input tensor `H` of shape `[B, T, D]` (batch √ó time steps √ó hidden dimension) and a tensor `lengths` indicating the actual sequence lengths (excluding padding), the module selects a subset of the most salient timesteps and computes their average representation.\n",
    "\n",
    "Operations:\n",
    "\n",
    "1. **Masking**: A boolean mask identifies valid (non-padded) positions using the lengths.\n",
    "\n",
    "2. **Scoring**: For each timestep, salience is measured as its L2 norm across the feature dimension (`H.norm(dim=2)`), resulting in a scalar salience score per timestep. Padded positions are assigned a very negative value (`-1e9`) so they are never selected.\n",
    "\n",
    "3. **Top-k selection**: For each sequence `b`, the number of top timesteps to select, `k_vals[b]`, is computed as `ceil(q * length[b])`, where `q` (default 0.15) is the fraction of timesteps to retain. This value is clamped to at least 1.\n",
    "\n",
    "4. **Pooling**: For each batch element, the indices of the top-`k_vals[b]` timesteps (by salience) are selected using `topk`. The corresponding hidden representations are averaged along the timestep dimension to produce a single pooled vector of size `D`.\n",
    "\n",
    "The output tensor has shape `[B, D]`, where each row is the mean of the top-`q` most salient timesteps according to their L2 norms.\n",
    "\n",
    "This operation focuses the representation on the most informative parts of each sequence, discarding padding and low-salience steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ab47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HALT(nn.Module):\n",
    "    \"\"\"\n",
    "    HALT: Hallucination Assessment via Log-probs as Time series\n",
    "    (Sec 3.2 + Appendix B)\n",
    "\n",
    "    Input:\n",
    "        x:       [B, T, D=25] padded feature sequences\n",
    "        lengths: [B]          original sequence lengths\n",
    "    Output:\n",
    "        logits:  [B]          raw hallucination scores (apply sigmoid)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: HALTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        self.projection = InputProjection(config.input_dim, config.proj_dim)\n",
    "\n",
    "        # Bidirectional GRU encoder (Appendix B)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=config.proj_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=config.dropout if config.num_layers > 1 else 0.0,\n",
    "        )\n",
    "        gru_out_dim = config.hidden_dim * 2   # 512\n",
    "        self.pooler = TopQPooling(q=config.top_q)\n",
    "        self.classifier = nn.Linear(gru_out_dim, 1)\n",
    "\n",
    "        # Proper weight initialization (Appendix B)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # GRU weights: xavier for input-hidden, orthogonal for hidden-hidden\n",
    "        for name, p in self.gru.named_parameters():\n",
    "            if \"weight_ih\" in name:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(p)\n",
    "            elif \"bias\" in name:\n",
    "                nn.init.zeros_(p)\n",
    "\n",
    "        # Classifier\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, _ = x.shape\n",
    "        proj    = self.projection(x)                          # [B,T,128]\n",
    "        lengths_cpu = lengths.cpu()  # pack_padded_sequence requires CPU\n",
    "\n",
    "        # üîë Sort lengths descending (required by pack_padded_sequence)\n",
    "        idx_sorted = torch.sort(lengths_cpu, descending=True).indices\n",
    "        lengths_sorted = lengths_cpu[idx_sorted]\n",
    "        proj_sorted    = proj[idx_sorted]\n",
    "\n",
    "        packed  = pack_padded_sequence(proj_sorted, lengths_sorted,\n",
    "                                       batch_first=True, enforce_sorted=True)\n",
    "        out_pk, _ = self.gru(packed)\n",
    "        H, _    = pad_packed_sequence(out_pk, batch_first=True,\n",
    "                                      total_length=T)        # [B,T,512]\n",
    "\n",
    "        # Restore original order for pooling & loss\n",
    "        inv_idx = idx_sorted.argsort()\n",
    "        H_orig  = H[inv_idx]\n",
    "\n",
    "        pooled  = self.pooler(H_orig, lengths)               # [B,512]\n",
    "        logits  = self.classifier(pooled).squeeze(1)         # [B]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538b276",
   "metadata": {},
   "source": [
    "The `HALT` class implements the full hallucination assessment model, as described in Section 3.2 and Appendix B.\n",
    "\n",
    "**Architecture overview**:  \n",
    "- Input: padded sequence `x` of shape `[B, T, 25]` and corresponding `lengths` tensor (actual unpadded lengths per sequence).  \n",
    "- Output: logits of shape `[B]`, representing unnormalized hallucination scores (sigmoid applied externally during inference).\n",
    "\n",
    "**Components**:  \n",
    "\n",
    "1. **Input Projection (`InputProjection`)**:  \n",
    "   Maps the 25-dimensional engineered features to a 128-dimensional space via LayerNorm ‚Üí Linear ‚Üí GELU ‚Üí Linear, preserving temporal structure.\n",
    "\n",
    "2. **Bidirectional GRU Encoder**:  \n",
    "   Processes sequences with a 5-layer bidirectional GRU (hidden size 256, dropout 0.4). Since `pack_padded_sequence` requires sorted sequences and the original order must be preserved after processing, the forward pass:  \n",
    "   - Sorts inputs by descending length on CPU (required for `pack_padded_sequence`),  \n",
    "   - Packs and passes through the GRU,  \n",
    "   - Unpacks back to full length (`[B, T, 512]` ‚Äî 2√ó256 for bidirectional output),  \n",
    "   - Restores original batch order via inverse indexing.\n",
    "\n",
    "3. **Top-q Pooling (`TopQPooling`)**:  \n",
    "   Selects the top 15% of timesteps (by L2 norm) per sequence and averages their GRU hidden states, producing a fixed-size `[B, 512]` representation.\n",
    "\n",
    "4. **Classifier**:  \n",
    "   A single linear layer maps the 512-dimensional pooled representation to a scalar logit.\n",
    "\n",
    "**Weight Initialization**:  \n",
    "- Input-to-hidden weights in the GRU use Xavier initialization, hidden-to-hidden weights use orthogonal initialization (to mitigate vanishing gradients), and biases are zero-initialized.  \n",
    "- The classifier weights use Xavier initialization; bias is zero if present.\n",
    "\n",
    "The model outputs raw logits, where higher values indicate stronger predicted hallucination likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b67db",
   "metadata": {},
   "source": [
    "## Training & Evaluation Utilities \n",
    "(Appendix B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, max_grad_norm: float):\n",
    "    model.train()\n",
    "    total_loss, n = 0.0, 0\n",
    "    for x_pad, labels, lengths in loader:\n",
    "        x_pad = x_pad.to(CFG.device)\n",
    "        labels = labels.to(CFG.device)\n",
    "        lengths = lengths.to(CFG.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_pad, lengths)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "    return total_loss / max(n, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571e77fe",
   "metadata": {},
   "source": [
    "The `train_epoch` function performs one full training pass over the dataset.\n",
    "\n",
    "- Sets the model to training mode (`model.train()`), enabling operations like dropout.\n",
    "- Initializes cumulative loss and sample counter.\n",
    "\n",
    "For each batch `(x_pad, labels, lengths)` from the data loader:\n",
    "- Moves all tensors to the configured device (GPU or CPU).\n",
    "- Clears previous gradients with `optimizer.zero_grad()`.\n",
    "- Computes model predictions (`logits`) by calling the model with padded inputs and lengths.\n",
    "- Calculates loss between logits and labels using the provided criterion (e.g., BCEWithLogitsLoss).\n",
    "- Performs backpropagation via `loss.backward()`.\n",
    "- Clips gradients to the specified maximum norm (`max_grad_norm = 1.0`) to prevent exploding gradients.\n",
    "- Updates model parameters via `optimizer.step()`.\n",
    "\n",
    "Accumulates the per-batch loss and increments the counter. Returns the average training loss over all batches.\n",
    "\n",
    "Note: Loss averaging uses `max(n, 1)` to avoid division by zero in edge cases where the loader yields no batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a85739",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    total_loss, n = 0.0, 0\n",
    "\n",
    "    for x_pad, labels, lengths in loader:\n",
    "        x_pad = x_pad.to(device)\n",
    "        labels = labels.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        logits = model(x_pad, lengths)\n",
    "        loss   = criterion(logits, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n += 1\n",
    "        all_logits.extend(logits.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().int().tolist())\n",
    "\n",
    "    probs    = torch.sigmoid(torch.tensor(all_logits)).numpy()\n",
    "    preds    = (probs >= threshold).astype(int)\n",
    "    labels_np = np.array(all_labels)\n",
    "\n",
    "    macro_f1  = f1_score(labels_np, preds, average=\"macro\", zero_division=0)\n",
    "    accuracy  = (preds == labels_np).mean()\n",
    "    try:\n",
    "        auroc   = roc_auc_score(labels_np, probs)\n",
    "    except ValueError:  # e.g., single class\n",
    "        auroc   = 0.5\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / max(n, 1),\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"auroc\": auroc,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf3d7e0",
   "metadata": {},
   "source": [
    "The `evaluate` function computes model performance metrics on a validation or test dataset in evaluation mode.\n",
    "\n",
    "- Disables gradient computation (`@torch.no_grad()`) and sets the model to evaluation mode (`model.eval()`), turning off dropout and other training-specific behaviors.\n",
    "\n",
    "- Iterates through the data loader, moving inputs, labels, and lengths to the specified device.\n",
    "- Computes logits and loss (without updating model weights).\n",
    "- Stores all logits and true labels on CPU as Python lists.\n",
    "\n",
    "After collecting all predictions:\n",
    "- Applies the sigmoid function to logits to obtain probabilities.\n",
    "- Converts probabilities to binary predictions using a threshold (default 0.5).\n",
    "- Computes four metrics:\n",
    "  - **Average loss** over the dataset,\n",
    "  - **Macro F1 score**, averaging per-class F1 and handling zero-division cases by returning 0,\n",
    "  - **AUROC** (Area Under the ROC Curve); defaults to 0.5 if evaluation is impossible (e.g., labels are all one class),\n",
    "  - **Accuracy**, the proportion of correct binary predictions.\n",
    "\n",
    "Returns a dictionary containing these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fe83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_halt(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    criterion,\n",
    "    cfg: HALTConfig,\n",
    ") -> float:\n",
    "    \"\"\"Training loop with early stopping & LR scheduling.\"\"\"\n",
    "    best_f1, best_state, patience_ctr = -float(\"inf\"), None, 0\n",
    "\n",
    "    for epoch in range(1, cfg.max_epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion,\n",
    "                                 cfg.max_grad_norm)\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, criterion, CFG.device)\n",
    "        scheduler.step(val_metrics[\"macro_f1\"])\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}/{cfg.max_epochs} | \"\n",
    "                f\"train_loss={train_loss:.4f}, val_loss={val_metrics['loss']:.4f} | \"\n",
    "                f\"macro-F1={val_metrics['macro_f1']:.4f}, AUROC={val_metrics['auroc']:.4f}\"\n",
    "            )\n",
    "\n",
    "        # Early stopping based on macro-F1\n",
    "        if val_metrics[\"macro_f1\"] > best_f1:\n",
    "            best_f1 = val_metrics[\"macro_f1\"]\n",
    "            best_state = {k: v.clone().cpu() for k, v in model.state_dict().items()}\n",
    "            patience_ctr = 0\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            if patience_ctr >= cfg.early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch}. Best val macro-F1: {best_f1:.4f}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return best_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7a8b5",
   "metadata": {},
   "source": [
    "The `train_halt` function executes the complete training loop for the HALT model, incorporating learning rate scheduling and early stopping.\n",
    "\n",
    "- Initializes tracking variables:  \n",
    "  - `best_f1` (best macro-F1 score seen, initialized to negative infinity),  \n",
    "  - `best_state` (to store the model weights corresponding to the best macro-F1),  \n",
    "  - `patience_ctr` (counts consecutive epochs without improvement).\n",
    "\n",
    "- For each epoch from 1 to `cfg.max_epochs`:  \n",
    "  - Trains for one epoch using `train_epoch`, passing the gradient norm limit from config.  \n",
    "  - Evaluates on the validation set using `evaluate`, returning metrics including loss and macro-F1.  \n",
    "  - Updates the learning rate scheduler based on validation macro-F1 (assumes a metric-aware scheduler like `ReduceLROnPlateau`).  \n",
    "  - Every epoch or every 5 epochs (and always at epoch 1), prints training progress: epoch number, training loss, validation loss, macro-F1, and AUROC.  \n",
    "  - Checks for improvement in macro-F1:  \n",
    "    - If improved, updates `best_f1`, saves a CPU copy of the current model state dictionary, and resets patience counter.  \n",
    "    - If not improved, increments patience; when it reaches `cfg.early_stop_patience` (15 by default), prints a message and breaks out of training loop.\n",
    "\n",
    "- After training, loads the best-performing model weights (from epoch with highest macro-F1) back into the model.  \n",
    "- Returns the best validation macro-F1 score observed.\n",
    "\n",
    "This ensures robust training with protection against overfitting via early stopping and adaptive learning rate control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f5451",
   "metadata": {},
   "source": [
    "## Inference & Interpretability \n",
    "(Appendix C.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908a350",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = (\n",
    "    [\"avg_logp\", \"rank_proxy\", \"h_overall\", \"h_alts\", \"delta_h_dec\"]\n",
    "    + [f\"logprob_{i+1}\" for i in range(20)]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def predict_hallucination(logprobs_matrix: np.ndarray, model: nn.Module,\n",
    "                          device: str = CFG.device, threshold: float = 0.5) -> dict:\n",
    "    \"\"\"\n",
    "    Predict hallucination for a single LLM response.\n",
    "\n",
    "    Args:\n",
    "        logprobs_matrix: [T, top_k] ‚Äî top-k log-probs per token.\n",
    "                         Column 0 = selected token, rest = alternatives sorted desc.\n",
    "\n",
    "    Returns:\n",
    "        dict with prob_hallucinated, prediction, logit\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    x = build_halt_input(torch.from_numpy(logprobs_matrix.astype(np.float32)))\n",
    "    x = x.unsqueeze(0).to(device)                      # [1,T,25]\n",
    "    lengths = torch.tensor([logprobs_matrix.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logit = model(x, lengths).item()\n",
    "    prob = torch.sigmoid(torch.tensor(logit)).item()\n",
    "\n",
    "    return {\"prob_hallucinated\": prob, \"prediction\": int(prob >= threshold), \"logit\": logit}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec580cf",
   "metadata": {},
   "source": [
    "The `predict_hallucination` function runs inference on a single LLM response to assess whether it contains hallucinations.\n",
    "\n",
    "**Input**:  \n",
    "- `logprobs_matrix`: a 2D NumPy array of shape `[T, top_k]`, where each row contains the top-`K` log-probabilities for a token, with column 0 holding the selected (sampled) token and columns 1 onward holding alternatives sorted in descending order.  \n",
    "- `model`: the trained HALT model.  \n",
    "- Optional arguments: computation device (`device`) and decision threshold (default 0.5).\n",
    "\n",
    "**Process**:  \n",
    "- Sets the model to evaluation mode.  \n",
    "- Converts the NumPy input to a PyTorch tensor of type float32 and passes it through `build_halt_input` to construct the full 25-dimensional input (5 engineered features + 20 log-probabilities).  \n",
    "- Adds a batch dimension to form shape `[1, T, 25]`.  \n",
    "- Constructs a length tensor with value `[T]` (the actual sequence length).  \n",
    "- Moves both tensors to the specified device.  \n",
    "- Disables gradient computation, runs forward pass through the model to obtain logits, and converts the scalar logit to a Python float.  \n",
    "- Applies sigmoid to convert the logit into a probability of hallucination.\n",
    "\n",
    "**Output**:  \n",
    "A dictionary with three keys:  \n",
    "- `\"prob_hallucinated\"`: estimated probability (float between 0 and 1),  \n",
    "- `\"prediction\"`: binary decision (0 for no hallucination, 1 for hallucination),  \n",
    "- `\"logit\"`: raw model output before sigmoid.  \n",
    "\n",
    "This enables straightforward interpretation and use of the model‚Äôs confidence and decision for individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14844707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, loader, device, n_features=25) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Feature importance via Gradient √ó Input (Appendix C.2).\n",
    "    Safely handles model grad state.\n",
    "    \"\"\"\n",
    "    # üîë Ensure model is in train mode AND parameters require grad\n",
    "    model.train()\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if not p.requires_grad:\n",
    "            print(\"‚ö†Ô∏è  Enabling grad on parameter:\", p.shape)\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    feat_imp = torch.zeros(n_features, device=device)\n",
    "\n",
    "    for x_pad, labels, lengths in loader:\n",
    "        # Ensure x_pad requires grad\n",
    "        x_pad = torch.nn.Parameter(x_pad.to(device), requires_grad=True)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(x_pad, lengths)  # [B]\n",
    "        \n",
    "        # Compute gradient of sum (scalar) w.r.t. inputs\n",
    "        loss = logits.sum()\n",
    "        grads = torch.autograd.grad(\n",
    "            outputs=loss,\n",
    "            inputs=x_pad,\n",
    "            retain_graph=False,\n",
    "            create_graph=False\n",
    "        )[0]\n",
    "\n",
    "        B, T, D = grads.shape\n",
    "        mask    = (torch.arange(T, device=device).unsqueeze(0) < lengths.unsqueeze(1)).unsqueeze(2)\n",
    "        grad_input = (grads.abs() * x_pad.abs()) * mask\n",
    "\n",
    "        feat_imp += grad_input.sum(dim=(0, 1))\n",
    "    \n",
    "    model.eval()\n",
    "    feat_imp /= feat_imp.sum() + EPS\n",
    "    importance_dict = dict(zip(FEATURE_NAMES, feat_imp.tolist()))\n",
    "    return dict(sorted(importance_dict.items(), key=lambda kv: kv[1], reverse=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578384f4",
   "metadata": {},
   "source": [
    "The `compute_feature_importance` function estimates feature importance using the *Gradient √ó Input* method, as described in Appendix C.2.\n",
    "\n",
    "- Sets the model to training mode and ensures all parameters have gradient computation enabled (`requires_grad=True`), as importance is derived from gradients w.r.t. inputs.\n",
    "\n",
    "- Initializes a tensor `feat_imp` of zeros (length 25) on the specified device to accumulate importance scores across batches.\n",
    "\n",
    "For each batch in the data loader:\n",
    "- Wraps the padded input tensor `x_pad` as a `torch.nn.Parameter` with `requires_grad=True`, enabling gradient computation.\n",
    "- Moves labels to the device and computes model logits.\n",
    "- Defines a scalar loss as the sum of logits, then computes gradients of this loss w.r.t. the input `x_pad` using `torch.autograd.grad`, producing a gradient tensor of shape `[B, T, D]`.\n",
    "\n",
    "- Constructs a boolean mask to identify valid (non-padded) timesteps using `lengths`.  \n",
    "- Computes element-wise absolute gradient √ó absolute input (`grad_input`), masked to exclude padding. This aligns with the Gradient √ó Input salience measure, prioritizing features whose gradient and activation magnitude jointly indicate contribution.\n",
    "\n",
    "- Aggregates `grad_input` across batch and time dimensions (sum over axes 0 and 1), accumulating per-feature importance.\n",
    "\n",
    "After processing all batches:\n",
    "- Sets the model back to evaluation mode.  \n",
    "- Normalizes accumulated importance scores so they sum to 1 (using `EPS = 1e-9` for numerical stability).  \n",
    "- Zips the normalized scores with `FEATURE_NAMES` (the ordered list of feature names: 5 engineered features followed by `logprob_1` through `logprob_20`) to form a dictionary.  \n",
    "- Sorts the dictionary in descending order by importance value and returns it.\n",
    "\n",
    "The result is a mapping from feature name to normalized importance score, allowing interpretation of which inputs most strongly influenced hallucination predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc0f07",
   "metadata": {},
   "source": [
    "## Shape sanity & backward pass check (debug/coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _shape_sanity_check(model, cfg: HALTConfig):\n",
    "    B_t, T_t = 4, 80\n",
    "    x   = torch.randn(B_t, T_t, cfg.input_dim).to(CFG.device)\n",
    "    lens = torch.tensor([80, 65, 40, 20], dtype=torch.long).to(CFG.device)\n",
    "    lbls = torch.zeros(B_t).to(CFG.device)\n",
    "\n",
    "    out = model(x, lens)\n",
    "    loss = nn.BCEWithLogitsLoss()(out, lbls)\n",
    "    loss.backward()\n",
    "\n",
    "    has_grads = all(p.grad is not None for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"  Input shape:   {list(x.shape)}\")\n",
    "    print(f\"  Output shape:  {list(out.shape)}\")\n",
    "    print(f\"  Loss:          {loss.item():.4f}\")\n",
    "    print(f\"  Gradients OK? {has_grads} ‚úì\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4cacbd",
   "metadata": {},
   "source": [
    "The `_shape_sanity_check` function verifies that the model, data pipeline, and loss computation are correctly integrated and support gradient flow.\n",
    "\n",
    "- Constructs a small dummy batch:  \n",
    "  - Input `x` of shape `[4, 80, input_dim]`, with random values,  \n",
    "  - Sequence `lengths` set to `[80, 65, 40, 20]` (decreasing to match the sorting requirement of `pack_padded_sequence`),  \n",
    "  - Binary labels initialized to zeros (`[4]`).\n",
    "\n",
    "- Runs the model on this batch to obtain outputs, computes binary cross-entropy loss between outputs and labels, then calls `loss.backward()` to propagate gradients.\n",
    "\n",
    "- Checks whether all trainable parameters (those with `requires_grad=True`) have non-`None` gradients after backpropagation.\n",
    "\n",
    "- Prints:  \n",
    "  - Input tensor shape,  \n",
    "  - Output tensor shape (should be `[4]`),  \n",
    "  - Computed loss value (rounded to four decimals),  \n",
    "  - A verification status indicating whether gradients were computed successfully.\n",
    "\n",
    "This check ensures the model runs end-to-end without shape mismatches, loss computation errors, or broken gradient paths, and confirms that the training setup is ready for actual use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179dd38b",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446a46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')  # Non-interactive backend\n",
    "\n",
    "def plot_feature_importance(\n",
    "    importance_dict: dict,\n",
    "    output_path: str = \"feature_importance.png\",\n",
    "    top_k: int = 10,\n",
    "    title: str = \"Feature Importance (Gradient √ó Input)\",\n",
    "):\n",
    "    \"\"\"Bar plot of feature importance.\"\"\"\n",
    "    features = list(importance_dict.keys())\n",
    "    scores   = list(importance_dict.values())\n",
    "\n",
    "    # Sort descending\n",
    "    indices = np.argsort(scores)[::-1][:top_k]\n",
    "    features_top = [features[i] for i in indices]\n",
    "    scores_top   = [scores[i] for i in indices]\n",
    "\n",
    "    # Normalize\n",
    "    scores_top = minmax_scale(scores_top, feature_range=(0.2, 1.0))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 5))\n",
    "    bars = ax.barh(features_top[::-1], scores_top[::-1],\n",
    "                   color=\"#2C7BB6\", edgecolor=\"black\", linewidth=0.5)\n",
    "\n",
    "    ax.set_xlabel(\"Normalized Importance Score\")\n",
    "    ax.set_title(title, fontsize=12, pad=10)\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "    ax.tick_params(axis='y', labelsize=10)\n",
    "\n",
    "    # Annotate\n",
    "    for i, v in enumerate(scores_top[::-1]):\n",
    "        ax.text(v + 0.02, i, f\"{v:.3f}\", va=\"center\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"‚úÖ Saved feature importance plot ‚Üí {output_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b02fa",
   "metadata": {},
   "source": [
    "The `plot_feature_importance` function creates a horizontal bar chart of the top-k features by importance.\n",
    "\n",
    "- Inputs:  \n",
    "  - `importance_dict`: dictionary mapping feature names to normalized scores,  \n",
    "  - `output_path`: filename for saving the plot (default `\"feature_importance.png\"`),  \n",
    "  - `top_k`: number of top features to display (default 10),  \n",
    "  - `title`: plot title.\n",
    "\n",
    "- It extracts keys and values, sorts features in descending order of importance, then selects the top `top_k`.  \n",
    "- Scores are rescaled to the range [0.2, 1.0] using min-max scaling for visual clarity‚Äîavoiding overly flat bars due to extreme concentration or dispersion.  \n",
    "- A horizontal bar chart is drawn: features ordered from highest (top) to lowest (bottom), with bars colored in blue (`#2C7BB6`) and black edges.  \n",
    "- Labels, title, and grid (horizontal) are added for readability; y-axis tick labels use font size 10.  \n",
    "- Each bar is annotated with its exact normalized score (three decimals), positioned slightly to the right of the bar.  \n",
    "- Layout is tightly adjusted, figure saved at 300 DPI with tight bounding box for quality and correctness of labeling.  \n",
    "- The figure is closed to free memory, and a success message prints.\n",
    "\n",
    "This visualization highlights the relative contribution of each input feature to hallucination prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f255117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_feature_correlation(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    output_path: str = \"feature_correlation.png\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Correlation heatmap between engineered features and model logits.\n",
    "    Uses gradient-safe input path via compute_feature_importance pattern.\n",
    "    \"\"\"\n",
    "    # Temporarily enable grad & eval to get logits\n",
    "    model.train()\n",
    "    X, Y = [], []\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        for x_pad, labels, lengths in loader:\n",
    "            # Wrap input in Parameter (like compute_feature_importance)\n",
    "            x_pad = torch.nn.Parameter(x_pad.to(device), requires_grad=True)\n",
    "            logits = model(x_pad, lengths)\n",
    "\n",
    "            # Use only first 5 engineered features (avg over time)\n",
    "            feat_means = x_pad[:, :, :5].mean(dim=1).detach().cpu().numpy()\n",
    "            X.append(feat_means)\n",
    "\n",
    "            Y.extend(logits.detach().cpu().numpy())\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    corr_matrix = []\n",
    "    for i in range(5):\n",
    "        feat_i = X[:, i]\n",
    "        corr = np.corrcoef(feat_i, Y)[0, 1]\n",
    "        corr_matrix.append(corr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 3.2))\n",
    "    im = ax.imshow([corr_matrix], cmap=\"RdBu_r\", vmin=-1, vmax=1)\n",
    "\n",
    "    ax.set_yticks([0])\n",
    "    ax.set_yticklabels([\"Logits\"], fontsize=10)\n",
    "    ax.set_xticks(range(5))\n",
    "    ax.set_xticklabels([\n",
    "        \"AvgLogP\", \"RankProxy\",\n",
    "        \"Hoverall\", \"Halts\", \"ŒîHdec\"\n",
    "    ], fontsize=10)\n",
    "    ax.set_title(\"Feature ‚Üî Hallucination Logits Correlation\", pad=8)\n",
    "\n",
    "    cbar = plt.colorbar(im, shrink=0.75)\n",
    "    cbar.set_label(\"Pearson Correlation\", fontsize=9)\n",
    "\n",
    "    # Annotations\n",
    "    for i in range(5):\n",
    "        color = \"black\" if abs(corr_matrix[i]) < 0.7 else \"white\"\n",
    "        ax.text(i, 0, f\"{corr_matrix[i]:+.2f}\", ha=\"center\", va=\"center\",\n",
    "                color=color, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    print(f\"‚úÖ Saved correlation heatmap ‚Üí {output_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ac09b",
   "metadata": {},
   "source": [
    "The `plot_feature_correlation` function generates a heatmap showing Pearson correlations between the five engineered features and model logits (hallucination scores).\n",
    "\n",
    "- In training mode with gradient enabled, it iterates through the data loader, wrapping inputs in `torch.nn.Parameter` to preserve gradient compatibility.\n",
    "- For each batch, it extracts the first five columns of the input tensor (engineered features: AvgLogP, RankProxy, h_overall, h_alts, delta_h_dec), averages them across time steps to obtain per-sample feature values, and collects logits.\n",
    "- After gathering all data, the model is set back to evaluation mode.\n",
    "\n",
    "A correlation matrix is computed: for each of the five engineered features, Pearson correlation with logits is calculated and stored.\n",
    "\n",
    "A 1√ó5 heatmap is plotted:\n",
    "- Uses diverging colormap (`RdBu_r`) with range [‚àí1, 1] for intuitive interpretation of positive/negative correlations.\n",
    "- Y-axis labels the single row as ‚ÄúLogits‚Äù.\n",
    "- X-axis labels each feature with abbreviated names (e.g., ‚ÄúHoverall‚Äù, ‚ÄúHalts‚Äù, ‚ÄúŒîHdec‚Äù).\n",
    "- A colorbar indicates correlation strength.\n",
    "- Correlation values are annotated on the heatmap: black text for low absolute correlation (|r| < 0.7), white for higher magnitude to ensure readability.\n",
    "\n",
    "The figure is saved at 300 DPI, and a confirmation message prints upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_embeddings_for_t_sne(model, loader, device, n_samples=2000):\n",
    "    \"\"\"\n",
    "    Extract embeddings ‚Äî only if they are detachable.\n",
    "    Uses same forward as compute_feature_importance to ensure consistency.\n",
    "    \"\"\"\n",
    "    model.train()  # Ensure grad enabled\n",
    "    all_embs, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():  # Safe: embeddings don‚Äôt need gradients\n",
    "        for x_pad, labels, lengths in loader:\n",
    "            # Use same input as feature importance: wrap in Parameter but detach\n",
    "            x_pad = torch.nn.Parameter(x_pad.to(device), requires_grad=False)  # no grad needed here\n",
    "            logits = model(x_pad, lengths)\n",
    "\n",
    "            # Extract pooled embeddings via same pipeline:\n",
    "            proj = model.projection(x_pad)\n",
    "            lengths_cpu = lengths.cpu()\n",
    "            idx_sorted  = torch.sort(lengths_cpu, descending=True).indices\n",
    "            proj_sorted = proj[idx_sorted]\n",
    "            lengths_sorted = lengths_cpu[idx_sorted]\n",
    "\n",
    "            packed  = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                proj_sorted, lengths_sorted, batch_first=True, enforce_sorted=True\n",
    "            )\n",
    "            out_pk, _ = model.gru(packed)\n",
    "            H, _    = torch.nn.utils.rnn.pad_packed_sequence(out_pk, batch_first=True)\n",
    "            inv_idx = idx_sorted.argsort()\n",
    "            H_orig  = H[inv_idx]\n",
    "\n",
    "            pooled = model.pooler(H_orig, lengths)\n",
    "            all_embs.append(pooled.detach().cpu().numpy())\n",
    "            # Use true labels from dataset (not prediction)\n",
    "            all_labels.extend(labels.int().cpu().tolist())\n",
    "\n",
    "            if len(all_embs) * loader.batch_size >= n_samples:\n",
    "                break\n",
    "\n",
    "    model.eval()\n",
    "    embeddings = np.vstack(all_embs)\n",
    "    labels_arr   = np.array(all_labels[:len(embeddings)])\n",
    "    return embeddings, labels_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3733c315",
   "metadata": {},
   "source": [
    "The `extract_embeddings_for_t_sne` function extracts contextual embeddings from the model for downstream dimensionality reduction (e.g., t-SNE), ensuring consistency with the model‚Äôs internal processing pipeline.\n",
    "\n",
    "- Sets the model to training mode (to match conditions used during gradient-based analyses, though `torch.no_grad()` is later applied since embeddings are not needed for backprop).\n",
    "- Iterates through the data loader, wrapping inputs as `Parameter` with gradient disabled (`requires_grad=False`) for safety.\n",
    "- Performs the same forward path used in `HALT.forward`:  \n",
    "  - Input projection (`projection` layer),  \n",
    "  - Sorting by sequence length and packing,  \n",
    "  - Bidirectional GRU encoding,  \n",
    "  - Unpacking and restoring original order,  \n",
    "  - Top-`q` pooling (via `model.pooler`) to obtain pooled representations.\n",
    "- Embeddings and true labels are collected on CPU in NumPy format.  \n",
    "- Loop breaks once at least `n_samples` (default 2000) samples are collected, in case the loader yields more.\n",
    "\n",
    "- Returns two NumPy arrays:  \n",
    "  - `embeddings`: shape `[N, D]` (D = 512),  \n",
    "  - `labels_arr`: binary labels matching the embedded samples.\n",
    "\n",
    "This ensures embeddings reflect exactly how they are used in the model, without modification or approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_t_sne_embeddings(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    output_path: str = \"tsne_embeddings.png\",\n",
    "    n_samples: int = 1500,\n",
    "    perplexity: float = 30.0\n",
    "):\n",
    "    \"\"\"\n",
    "    T-SNE visualization of GRU embeddings, colored by **true class** (hallucinated vs correct).\n",
    "    Uses same data loading and embedding extraction logic as feature importance.\n",
    "    \"\"\"\n",
    "    print(f\"üß† Extracting {n_samples} embeddings for T-SNE...\")\n",
    "    embeddings, labels = extract_embeddings_for_t_sne(model, loader, device, n_samples=n_samples)\n",
    "\n",
    "    # Subsample if needed (T-SNE O(N^2))\n",
    "    if len(embeddings) > n_samples:\n",
    "        idx = np.random.choice(len(embeddings), size=n_samples, replace=False)\n",
    "        embeddings = embeddings[idx]\n",
    "        labels     = labels[idx]\n",
    "\n",
    "    # T-SNE\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity,\n",
    "                random_state=42, max_iter=500, metric=\"euclidean\")\n",
    "    embs_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 5))\n",
    "\n",
    "    class_names = [\"Hallucinated\", \"Correct\"]\n",
    "    colors      = [\"#D7191C\", \"#2B83BA\"]\n",
    "\n",
    "    for cls in [0, 1]:\n",
    "        mask = labels == cls\n",
    "        ax.scatter(\n",
    "            embs_2d[mask, 0], embs_2d[mask, 1],\n",
    "            c=colors[cls], label=class_names[cls],\n",
    "            alpha=0.6, s=25, edgecolor=\"none\"\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"T-SNE Component 1\", fontsize=10)\n",
    "    ax.set_ylabel(\"T-SNE Component 2\", fontsize=10)\n",
    "    ax.set_title(\"GRU Embedding Space (T-SNE Projection)\", fontsize=12, pad=10)\n",
    "    ax.legend(loc=\"best\", fontsize=9)\n",
    "\n",
    "    # Remove ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"‚úÖ Saved T-SNE plot ‚Üí {output_path}\")\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e9c7a",
   "metadata": {},
   "source": [
    "The `plot_t_sne_embeddings` function visualizes the model‚Äôs GRU-derived embeddings in 2D using t-SNE, colored by ground-truth class (hallucinated vs. correct).\n",
    "\n",
    "- Calls `extract_embeddings_for_t_sne` to obtain embeddings and true labels for up to `n_samples` (default 1500). If more are collected, random subsampling is applied to match `n_samples`, ensuring computational feasibility for t-SNE (quadratic complexity).\n",
    "\n",
    "- Applies t-SNE with:  \n",
    "  - 2 components,  \n",
    "  - Perplexity (default 30.0),  \n",
    "  - Fixed random seed (42) for reproducibility,  \n",
    "  - Maximum 500 iterations,  \n",
    "  - Euclidean distance metric.\n",
    "\n",
    "- Produces a scatter plot:  \n",
    "  - Hallucinated samples (label=1) in red (`#D7191C`),  \n",
    "  - Correct samples (label=0) in blue (`#2B83BA`).  \n",
    "  - Points are semi-transparent (alpha=0.6) with size 25 and no edge color.\n",
    "\n",
    "- Axis labels and title describe the visualization; legend is placed automatically. Ticks are removed for clarity.\n",
    "\n",
    "- The figure is saved at 300 DPI with tight bounding box, and a success message prints.\n",
    "\n",
    "This plot provides intuition about whether hallucinated and correct samples separate in the embedding space, revealing model behavior at the representation level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_sensitivity_analysis(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    n_samples: int = 50,\n",
    "    output_path: str = \"sensitivity_analysis.png\"\n",
    "):\n",
    "    \"\"\"SHAP-like perturbation sensitivity on top-5 engineered features.\"\"\"\n",
    "    model.train()\n",
    "\n",
    "    results = {f: [] for f in range(25)}\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        for x_pad, _, lengths in loader:\n",
    "            if len(results[0]) >= n_samples * 5:\n",
    "                break\n",
    "\n",
    "            # Detach & clone to avoid interfering with training\n",
    "            x_clean = torch.nn.Parameter(x_pad.to(device).detach().clone(), requires_grad=False)\n",
    "\n",
    "            # Compute baseline logits (without perturbation)\n",
    "            with torch.no_grad():\n",
    "                base_logits = model(x_pad.to(device), lengths.to(device))\n",
    "\n",
    "            for feat_idx in range(5):\n",
    "                # Create perturbed copy\n",
    "                x_pert = x_pad.clone().detach()\n",
    "                mean_feat = x_pert[:, :, feat_idx].mean(dim=1, keepdim=True)\n",
    "                std_feat  = x_pert[:, :, feat_idx].std(dim=1, keepdim=True) + 1e-8\n",
    "                noise = torch.randn_like(x_pert[:, :, feat_idx]) * std_feat * 0.5\n",
    "                x_pert[:, :, feat_idx] += noise\n",
    "\n",
    "                # Perturbed logits (no grad needed for sensitivity)\n",
    "                with torch.no_grad():\n",
    "                    pert_logits = model(x_pert.to(device), lengths.to(device))\n",
    "\n",
    "                delta_logits = torch.abs(pert_logits - base_logits).cpu().numpy()\n",
    "                results[feat_idx].extend(delta_logits.tolist())\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mean_delta = [np.mean(results[i]) for i in range(5)]\n",
    "    std_delta  = [np.std(results[i]) for i in range(5)]\n",
    "\n",
    "    features_names = [\"AvgLogP\", \"RankProxy\", \"Hoverall\", \"Halts\", \"ŒîHdec\"]\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    x_pos = np.arange(len(features_names))\n",
    "    ax.bar(x_pos, mean_delta, yerr=std_delta,\n",
    "           color=\"#1B9E77\", capsize=5, alpha=0.8, edgecolor=\"black\")\n",
    "\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(features_names, fontsize=10)\n",
    "    ax.set_ylabel(\"Logit Œî (Perturbation Sensitivity)\", fontsize=10)\n",
    "    ax.set_title(\"Sensitivity Analysis: Engineered Features\", pad=8)\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    print(f\"‚úÖ Saved sensitivity plot ‚Üí {output_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d615dfcb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35cc4545",
   "metadata": {},
   "source": [
    "# Run Forrest run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbee1ea",
   "metadata": {},
   "source": [
    "## Prep work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daabfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = HALTConfig()\n",
    "print(f\"\\n‚öôÔ∏è Configuration:\\n{cfg}\")\n",
    "print(f\"üíæ Device: {CFG.device}\")\n",
    "\n",
    "# ‚îÄ‚îÄ Dataset\n",
    "full_ds = SyntheticLogProbDataset(n_samples=4000, top_k=cfg.top_k, seed=SEED)\n",
    "n_total = len(full_ds)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val   = int(0.15 * n_total)\n",
    "n_test  = n_total - n_train - n_val\n",
    "\n",
    "train_set, val_set, test_set = random_split(\n",
    "    full_ds,\n",
    "    [n_train, n_val, n_test],\n",
    "    generator=torch.Generator().manual_seed(SEED),\n",
    ")\n",
    "\n",
    "loader_kw = dict(collate_fn=collate_fn, num_workers=0)\n",
    "train_loader = DataLoader(train_set, batch_size=cfg.batch_size,\n",
    "                            shuffle=True,  **loader_kw)\n",
    "val_loader   = DataLoader(val_set,   batch_size=cfg.batch_size,\n",
    "                            shuffle=False, **loader_kw)\n",
    "test_loader  = DataLoader(test_set,  batch_size=cfg.batch_size,\n",
    "                            shuffle=False, **loader_kw)\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  Train: {len(train_set):>5} | Val: {len(val_set):>5} | Test: {len(test_set):>5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53d6a4",
   "metadata": {},
   "source": [
    "Configuration is instantiated from `HALTConfig`, and key settings‚Äîincluding computing device‚Äîare printed.\n",
    "\n",
    "A synthetic dataset of 4000 samples is generated using `SyntheticLogProbDataset`. The dataset is split into training (70%), validation (15%), and test (15%) subsets using `random_split`, with shuffling controlled by a deterministic random generator seeded at 42 to ensure reproducibility.\n",
    "\n",
    "Data loaders are created for each subset:\n",
    "- Training loader uses `shuffle=True` to randomize batches.\n",
    "- Validation and test loaders use `shuffle=False` to maintain consistent ordering.\n",
    "- All loaders use the previously defined `collate_fn` and disable multiprocessing (`num_workers=0`).\n",
    "\n",
    "Finally, the sizes of the train/validation/test splits are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f124ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HALT(cfg).to(CFG.device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nüß† Model: HALT ({n_params:,} params ‚âà {n_params/1e6:.1f}M)\")\n",
    "\n",
    "# ‚îÄ‚îÄ Optimizer & scheduler\n",
    "criterion = nn.BCEWithLogitsLoss().to(CFG.device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=cfg.lr,\n",
    "                                weight_decay=cfg.weight_decay)\n",
    "scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=cfg.lr_factor,\n",
    "    patience=cfg.lr_patience,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb49e3b",
   "metadata": {},
   "source": [
    "The HALT model is instantiated with the configuration and moved to the configured device (`CFG.device`). All parameters are explicitly set to require gradients (though this is typically redundant after initialization, it ensures no gradient-disabled layers remain).\n",
    "\n",
    "The total number of trainable parameters is counted and printed in both raw count and millions (e.g., \"1.2M\").\n",
    "\n",
    "The loss function is set to `BCEWithLogitsLoss`, appropriate for binary classification with raw logits. It is moved to the device.\n",
    "\n",
    "The optimizer is Adam, configured with:\n",
    "- Learning rate from config (`lr = 4.41e-4`),\n",
    "- L2 weight decay (`weight_decay = 2.34e-6`).\n",
    "\n",
    "The learning rate scheduler is `ReduceLROnPlateau`, which:\n",
    "- Monitors validation macro-F1 (`mode=\"max\"`),\n",
    "- Reduces learning rate by a factor of `lr_factor` (0.5) when no improvement occurs for `lr_patience` (3) epochs.\n",
    "\n",
    "This setup prepares the model and training infrastructure for the main training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8614a9b1",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93e1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"üöÄ Training HALT\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "best_val_f1 = train_halt(model, train_loader, val_loader,\n",
    "                            optimizer, scheduler, criterion, cfg)\n",
    "\n",
    "print(f\"\\n Best validation macro-F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5df90",
   "metadata": {},
   "source": [
    "\n",
    "The `train_halt` function is called with the model, data loaders, optimizer, learning rate scheduler, loss function, and configuration. It runs the full training loop‚Äîincluding early stopping based on validation macro-F1‚Äîand returns the highest macro-F1 score observed on the validation set.\n",
    "\n",
    "After training completes, the best validation macro-F1 is printed with four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final test evaluation\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"üß™ Final Test Evaluation\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "test_metrics = evaluate(model, test_loader, criterion, CFG.device)\n",
    "print(f\"  Macro-F1 : {test_metrics['macro_f1']:.4f}\")\n",
    "print(f\"  AUROC    : {test_metrics['auroc']:.4f}\")\n",
    "print(f\"  Accuracy : {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Loss     : {test_metrics['loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48920ad4",
   "metadata": {},
   "source": [
    "After training completes, a final evaluation is performed on the test set.\n",
    "\n",
    "A header signals the start of final test evaluation. The `evaluate` function is called with the trained model (which now holds the weights from its best validation macro-F1 checkpoint), the test data loader, loss function, and device.\n",
    "\n",
    "The resulting test metrics are printed: macro-F1 score, AUROC, accuracy, and average loss‚Äîeach formatted to four decimal places for precise reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698c0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ Example inference\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"üîç Example Inference\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "rng_demo = np.random.default_rng(123)\n",
    "for label_str, is_hall in [(\"Hallucinated\", True), (\"Correct\", False)]:\n",
    "    demo = _synthetic_logprobs(60, cfg.top_k, is_hall, rng_demo)\n",
    "    res  = predict_hallucination(demo, model, CFG.device)\n",
    "    pred_str = \"HALLUCINATED\" if res[\"prediction\"] else \"NOT HALLUCINATED\"\n",
    "    print(f\"[{label_str:>13}]  P(hallucinated)={res['prob_hallucinated']:.4f} \"\n",
    "            f\"‚Üí {pred_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5273d83b",
   "metadata": {},
   "source": [
    "A demo inference section is run to illustrate model behavior on synthetic examples.\n",
    "\n",
    "A NumPy random generator seeded at 123 is created for reproducibility. For two cases‚Äîmarked as \"Hallucinated\" and \"Correct\"‚Äîa 60-step synthetic log-probability sequence is generated using `_synthetic_logprobs`, with the `hallucinated` flag set appropriately.\n",
    "\n",
    "Each sequence is passed to `predict_hallucination`, which returns the hallucination probability, binary prediction, and logit.\n",
    "\n",
    "The output is formatted to show:\n",
    "- The ground-truth label (\"Hallucinated\" or \"Correct\"),\n",
    "- The predicted probability of hallucination (four decimals),\n",
    "- The resulting prediction (\"HALLUCINATED\" or \"NOT HALLUCINATED\").\n",
    "\n",
    "This provides intuitive verification that the model assigns high hallucination probability to synthetic hallucinated sequences and low probability to correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376abea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ Feature importance (Appendix C.2)\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"üìä Feature Importance (Gradient√óInput, top-10)\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "print(\"üîç Testing gradient flow...\")\n",
    "dummy_x = torch.randn(2, 5, cfg.input_dim, device=CFG.device, requires_grad=True)\n",
    "dummy_lens = torch.tensor([5, 3], device=CFG.device)\n",
    "\n",
    "model.eval()\n",
    "logits = model(dummy_x, dummy_lens)\n",
    "print(f\" logits.requires_grad? {logits.requires_grad}\")\n",
    "loss = nn.BCEWithLogitsLoss()(logits, torch.zeros(2, device=CFG.device))\n",
    "loss.backward()\n",
    "print(f\"dummy x.grad exists? {dummy_x.grad is not None}\")\n",
    "assert dummy_x.grad is not None, \"Break in gradient flow!\"\n",
    "print(\"‚úÖ Gradient test PASSED\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8364c3af",
   "metadata": {},
   "source": [
    "A feature importance analysis is prepared, preceded by a gradient flow validation step.\n",
    "\n",
    "- A small dummy input tensor (`dummy_x`) of shape `[2, 5, 25]` is created with `requires_grad=True`, and corresponding lengths `[5, 3]`.\n",
    "- The model is switched to evaluation mode.\n",
    "- Forward pass computes logits; binary cross-entropy loss is computed against zero labels.\n",
    "- Backward propagation is performed.\n",
    "\n",
    "The script checks that:\n",
    "- Output logits has `requires_grad=True`, confirming the forward path preserves gradients,\n",
    "- Input tensor `dummy_x` has a non-null gradient after backpropagation.\n",
    "\n",
    "An assertion ensures gradients are computed; if not, training would be impossible. A confirmation message is printed upon success.\n",
    "\n",
    "This pre-check verifies that the model supports gradient-based feature importance computation before proceeding with `compute_feature_importance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importance = compute_feature_importance(model, val_loader, CFG.device,\n",
    "                                        n_features=cfg.input_dim)\n",
    "print(f\"{'Feature':<22}  {'Importance':>10}\")\n",
    "print(\"-\" * 36)\n",
    "for name, score in list(importance.items())[:10]:\n",
    "    print(f\"{name:<22}  {score:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e02fea",
   "metadata": {},
   "source": [
    "Feature importance is computed on the validation set using gradient-based analysis (Gradient √ó Input).\n",
    "\n",
    "The `compute_feature_importance` function is called with the trained model, validation loader, device, and feature count (25). The function aggregates per-feature gradient magnitudes weighted by input magnitude across all non-padded timesteps and normalizes them to sum to 1.\n",
    "\n",
    "The top 10 most important features are printed in descending order of importance:\n",
    "- Each line shows the feature name (left-aligned, up to 22 characters) and its normalized importance score (right-aligned, formatted to four decimal places).  \n",
    "- A separator line precedes the output for readability.\n",
    "\n",
    "This highlights which engineered or raw log-probability features contribute most to hallucination detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b033469",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚îÄ‚îÄ Shape & backward pass check\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"üîç Shape Sanity Check & Backward Pass\")\n",
    "print(\"=\" * 72)\n",
    "_shape_sanity_check(model, cfg)\n",
    "\n",
    "# ‚îÄ‚îÄ Save model (optional)\n",
    "torch.save(model.state_dict(), \"halt_model.pt\")\n",
    "print(\"\\nüíæ Model saved to halt_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249e493",
   "metadata": {},
   "source": [
    "A final shape and gradient flow verification is performed by calling `_shape_sanity_check`, which tests that the model processes a small batch correctly, produces valid outputs, computes loss without errors, and successfully propagates gradients to all parameters.\n",
    "\n",
    "After confirming correctness, the trained model‚Äôs state dictionary is saved to disk as `\"halt_model.pt\"` using `torch.save`.\n",
    "\n",
    "A confirmation message indicates successful model persistence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a184b08",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c431bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = compute_feature_importance(model, val_loader, CFG.device)\n",
    "plot_feature_importance(importance, output_path=\"feature_importance.png\")\n",
    "\n",
    "plot_feature_correlation(model, val_loader, CFG.device)\n",
    "\n",
    "\n",
    "plot_t_sne_embeddings(\n",
    "    model, val_loader, CFG.device,\n",
    "    output_path=\"tsne_embeddings.png\",\n",
    "    n_samples=1500\n",
    ")\n",
    "\n",
    "plot_sensitivity_analysis(model, val_loader, CFG.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a9fda",
   "metadata": {},
   "source": [
    "Additional diagnostic visualizations are generated to analyze model behavior:\n",
    "\n",
    "- **Feature importance plot**: The `plot_feature_importance` function is called with the computed importance dictionary and saves a figure as `\"feature_importance.png\"`.\n",
    "\n",
    "- **Feature correlation analysis**: The `plot_feature_correlation` function is invoked to visualize pairwise correlations among engineered features and raw log-probabilities.\n",
    "\n",
    "- **t-SNE embedding visualization**: The `plot_t_sne_embeddings` function generates a 2D t-SNE plot of model representations for validation samples (up to 1500 samples), saving the result as `\"tsne_embeddings.png\"`. This helps assess whether hallucinated and correct samples separate in the embedding space.\n",
    "\n",
    "- **Sensitivity analysis**: The `plot_sensitivity_analysis` function is called to evaluate how input perturbations affect predictions‚Äîlikely assessing robustness or identifying vulnerable features.\n",
    "\n",
    "Each function produces a PNG file, enabling visual interpretation of model characteristics beyond scalar metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
