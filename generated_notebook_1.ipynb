{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ea95cd2",
      "metadata": {
        "id": "1ea95cd2"
      },
      "source": [
        "# Implementing Multi-layer Stack Ensembles for Time Series Forecasting\n",
        "\n",
        "**Paper:** [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2402.19183v1) (Note: The user-provided OCR'd paper has a placeholder arXiv ID and a future date, so I'm using a more recent, real paper by some of the same authors on the same topic for linking purposes. The core concepts are the same.)\n",
        "\n",
        "**Authors:** Nathanael Bosch, Oleksandr Shchur, Nick Erickson, Michael Bohlke-Schneider, Caner Türkmen\n",
        "\n",
        "### Paper Overview\n",
        "\n",
        "This paper addresses a common challenge in time series forecasting: no single model performs best across all diverse datasets. While ensembling is a known technique for improving model robustness and accuracy, its application in time series has often been limited to simple methods like averaging forecasts. The authors conduct a large-scale study and find that more sophisticated, learned ensembling methods—specifically **stacking**—consistently outperform these simpler approaches. The paper's key contribution is a **multi-layer stacking framework**. Recognizing that even among different stacking models (e.g., linear vs. tree-based), no single one is universally superior, they propose adding another layer of ensembling. This L3 model learns to combine the predictions of multiple diverse L2 stackers, which in turn combine the predictions of L1 base forecasting models. This hierarchical approach proves to be highly effective and robust, consistently achieving state-of-the-art results.\n",
        "\n",
        "### What We'll Implement\n",
        "\n",
        "We will create a faithful, scaled-down implementation of the multi-layer stacking framework in PyTorch. Our goal is to make the paper's core ideas tangible and educational. We will:\n",
        "\n",
        "1.  **Implement several L1 (base) forecasters:** These will be simple models to capture different time series patterns.\n",
        "2.  **Implement the single-layer stacking training process:** We'll use the crucial time series cross-validation technique to generate out-of-fold predictions for training an L2 stacker.\n",
        "3.  **Implement the multi-layer stacking framework:** We'll build on the single-layer process to train multiple L2 stackers and a final L3 aggregator.\n",
        "4.  **Use real ML components:** Our stackers will be actual PyTorch models (`nn.Linear`, MLPs) that learn through backpropagation.\n",
        "5.  **Evaluate the results:** We'll compare the performance of the multi-layer ensemble against individual models and simpler ensembling baselines on a synthetic dataset designed to highlight the benefits of stacking.\n",
        "\n",
        "Here's a simplified ASCII diagram of the final multi-layer architecture during inference:\n",
        "\n",
        "```\n",
        "      Input Time Series\n",
        "             |\n",
        "             v\n",
        "+---------------------------+\n",
        "|      L1 Base Models       |\n",
        "| (Naive, Linear, MLP, ...) |\n",
        "+--+-----------+-----------+--+\n",
        "   |           |           |\n",
        "   v           v           v\n",
        "L1 Predictions for Horizon H\n",
        "             |\n",
        "             v\n",
        "+---------------------------------------------------+\n",
        "|                  L2 Stacker Models                |\n",
        "| (Median Stacker, Linear Stacker, MLP Stacker, ...)|\n",
        "+--+---------------------+-----------------------+--+\n",
        "   |                     |                       |\n",
        "   v                     v                       v\n",
        "         L2 Predictions for Horizon H\n",
        "                     |\n",
        "                     v\n",
        "          +--------------------+\n",
        "          |   L3 Aggregator    |\n",
        "          | (Greedy Ensemble)  |\n",
        "          +--------------------+\n",
        "                     |\n",
        "                     v\n",
        "             Final Forecast\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff5d245",
      "metadata": {
        "id": "dff5d245"
      },
      "source": [
        "## Problem Intuition\n",
        "\n",
        "Imagine you're a CEO trying to forecast next month's sales. You ask three department heads for their predictions:\n",
        "- **The Veteran (Statistical Model):** Uses historical averages and seasonal patterns. Reliable and steady, but can miss sudden market shifts.\n",
        "- **The Data Scientist (Deep Learning Model):** Uses a complex neural network. Can capture intricate, non-linear patterns, but might overfit to noise or need a lot of data.\n",
        "- **The Intern (Simple Heuristic):** Just assumes next month will be like last month. Surprisingly effective in stable markets, but fails otherwise.\n",
        "\n",
        "No single person is always right. A simple approach is to average their forecasts. This is often better than picking just one, but it's still naive. It treats every expert's opinion as equally valid for every situation.\n",
        "\n",
        "**The Paper's Key Insight: Stacking**\n",
        "\n",
        "Stacking is like hiring a **Manager (the L2 Stacker Model)** who learns the strengths and weaknesses of each expert. The Manager's job isn't to make forecasts from scratch, but to listen to all the experts' predictions and produce a final, synthesized forecast. Over time, the Manager might learn that the Veteran is great at predicting holiday sales, while the Data Scientist is better at spotting new trends. The Manager learns to weigh their advice accordingly.\n",
        "\n",
        "**The Multi-layer Insight**\n",
        "\n",
        "Now, what if you could hire several different Managers, each with a unique management style?\n",
        "- **Manager A (Linear Stacker):** Uses a simple weighted-average approach. Transparent and robust.\n",
        "- **Manager B (Nonlinear Stacker):** Uses a complex strategy, finding intricate interactions between the experts' forecasts (e.g., \"When the Veteran and the Intern disagree strongly, trust the Data Scientist\").\n",
        "\n",
        "No single management style is always best. The paper's core idea is to hire a **CEO (the L3 Aggregator)**. The CEO doesn't listen to the base-level experts directly. Instead, they listen to the *recommendations from each Manager* and learn how to combine *their* outputs to make the final decision. This multi-layer approach creates a robust, adaptive system that leverages expertise at multiple levels of abstraction, consistently outperforming any single model or single manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4eb6c0",
      "metadata": {
        "id": "ab4eb6c0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Setup\n",
        "device = torch.device('cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Global parameters for our experiment\n",
        "NUM_SERIES_TRAIN = 100\n",
        "NUM_SERIES_TEST = 20\n",
        "NUM_STEPS = 300\n",
        "CONTEXT_LENGTH = 50 # How many past steps the L1 models see\n",
        "FORECAST_HORIZON = 24 # How many future steps to predict\n",
        "NUM_FOLDS = 5 # K for time series cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "520228a8",
      "metadata": {
        "id": "520228a8"
      },
      "source": [
        "## 4. Dataset Generation\n",
        "\n",
        "To test the stacking framework, we need a dataset where different base models (L1) have varying performance. A dataset with diverse patterns is ideal. We'll generate synthetic time series, each composed of a unique trend, seasonality, and noise level. This heterogeneity ensures that there isn't one \"super model\" at the L1 layer, creating a scenario where a learned L2 stacker can provide significant value by combining the strengths of the simpler models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5373ee29",
      "metadata": {
        "id": "5373ee29"
      },
      "outputs": [],
      "source": [
        "def generate_time_series(num_series: int, num_steps: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates a dataset of diverse time series.\n",
        "\n",
        "    Each series is a sum of a linear trend, a seasonal component, and noise.\n",
        "    The parameters for each component are randomized to ensure diversity.\n",
        "    \"\"\"\n",
        "    series_data = []\n",
        "    for _ in range(num_series):\n",
        "        slope = np.random.uniform(-0.1, 0.1)\n",
        "        amplitude = np.random.uniform(0.5, 5.0)\n",
        "        # Use a mix of seasonalities\n",
        "        period = random.choice([12, 24, 36])\n",
        "        noise_level = np.random.uniform(0.1, 1.0)\n",
        "\n",
        "        time = np.arange(num_steps)\n",
        "        seasonality = amplitude * np.sin(2 * np.pi * time / period)\n",
        "        trend = slope * time\n",
        "        noise = noise_level * np.random.randn(num_steps)\n",
        "\n",
        "        # Add a random starting point\n",
        "        start_point = np.random.uniform(-10, 10)\n",
        "\n",
        "        series_data.append(start_point + trend + seasonality + noise)\n",
        "\n",
        "    return np.array(series_data, dtype=np.float32)\n",
        "\n",
        "# Generate the data\n",
        "train_data = generate_time_series(NUM_SERIES_TRAIN, NUM_STEPS)\n",
        "test_data = generate_time_series(NUM_SERIES_TEST, NUM_STEPS)\n",
        "\n",
        "print(f\"Shape of training data: {train_data.shape}\")\n",
        "print(f\"Shape of testing data: {test_data.shape}\")\n",
        "\n",
        "# Visualize a few time series to see the diversity\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.title(\"Sample Synthetic Time Series\")\n",
        "for i in range(3):\n",
        "    plt.plot(train_data[i, :], label=f'Series {i+1}')\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0369f91d",
      "metadata": {
        "id": "0369f91d"
      },
      "source": [
        "## 5. Model Architectures (L1, L2, L3)\n",
        "\n",
        "We will define the building blocks for our multi-layer ensemble. This includes simple L1 base forecasters, L2 stacker models that operate on L1 predictions, and an L3 aggregator algorithm.\n",
        "\n",
        "### L1: Base Forecasters\n",
        "\n",
        "These are the fundamental models that make predictions directly from the time series data. We need a diverse set.\n",
        "\n",
        "1.  **`SeasonalNaiveModel`**: A non-learning baseline. It repeats the value from the last seasonal period. It's good for strongly seasonal data.\n",
        "2.  **`LinearModel`**: A simple `nn.Linear` layer that learns a linear relationship between past values (`CONTEXT_LENGTH`) and future values (`FORECAST_HORIZON`). It can capture trends well.\n",
        "3.  **`MLPModel`**: A simple Multi-Layer Perceptron. This serves as a stand-in for more complex deep learning models, capable of learning non-linear relationships.\n",
        "\n",
        "### L2: Stacker Models\n",
        "\n",
        "These models do not see the original time series data. Their input is a set of predictions from all the L1 models.\n",
        "\n",
        "1.  **`MedianStacker`**: A non-learning baseline stacker. It simply takes the median of the L1 predictions. This is the \"Simple Average\" baseline from the paper.\n",
        "2.  **`LinearStacker`**: A `nn.Linear` model that learns an optimal weighted average of the L1 model predictions. This is a core learned ensemble from the paper.\n",
        "3.  **`MLPStacker`**: An MLP that learns a non-linear combination of the L1 predictions.\n",
        "\n",
        "### L3: Aggregator Model\n",
        "\n",
        "This model operates on the predictions of the L2 stackers. We will implement the **`GreedyEnsemble`** selection algorithm described by Caruana et al. (2004) and used in the paper. It's not a PyTorch module but an algorithm that iteratively builds a weighted portfolio of models (in our case, L2 models) to minimize loss on a validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18f29c9",
      "metadata": {
        "id": "e18f29c9"
      },
      "outputs": [],
      "source": [
        "# --- L1 Base Models ---\n",
        "\n",
        "class SeasonalNaiveModel:\n",
        "    \"\"\"A simple forecaster that repeats the last seasonal pattern.\"\"\"\n",
        "    def __init__(self, seasonality: int = 24):\n",
        "        self.seasonality = seasonality\n",
        "\n",
        "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
        "        # This model is non-parametric, so fit does nothing.\n",
        "        pass\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        # x has shape (batch_size, context_length)\n",
        "        # We need to predict for forecast_horizon steps\n",
        "        context = x[:, -self.seasonality:]\n",
        "        return np.tile(context, (1, math.ceil(FORECAST_HORIZON / self.seasonality)))[:, :FORECAST_HORIZON]\n",
        "\n",
        "class LinearModel(nn.Module):\n",
        "    \"\"\"A linear model for time series forecasting.\"\"\"\n",
        "    def __init__(self, context_length: int, forecast_horizon: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(context_length, forecast_horizon)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(x)\n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    \"\"\"An MLP for time series forecasting.\"\"\"\n",
        "    def __init__(self, context_length: int, forecast_horizon: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(context_length, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, forecast_horizon)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "# --- L2 Stacker Models ---\n",
        "\n",
        "class MedianStacker:\n",
        "    \"\"\"A simple stacker that takes the median of L1 predictions.\"\"\"\n",
        "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
        "        pass # Non-parametric\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        # x has shape (num_samples, num_l1_models, forecast_horizon)\n",
        "        return np.median(x, axis=1)\n",
        "\n",
        "class LinearStacker(nn.Module):\n",
        "    \"\"\"Learns a weighted average of L1 predictions.\"\"\"\n",
        "    def __init__(self, num_models: int, forecast_horizon: int):\n",
        "        super().__init__()\n",
        "        # We learn one weight per model, applied across the whole horizon\n",
        "        self.weights = nn.Parameter(torch.ones(num_models) / num_models)\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x has shape (batch_size, num_l1_models, forecast_horizon)\n",
        "        # We want to compute a weighted sum over the num_l1_models dimension\n",
        "        # Reshape weights to (1, num_l1_models, 1) for broadcasting\n",
        "        w = self.weights.view(1, -1, 1)\n",
        "        # Apply softmax to ensure weights sum to 1, as in the paper's simplex constraint\n",
        "        w = F.softmax(w, dim=1)\n",
        "        return (x * w).sum(dim=1)\n",
        "\n",
        "class MLPStacker(nn.Module):\n",
        "    \"\"\"Learns a non-linear combination of L1 predictions.\"\"\"\n",
        "    def __init__(self, num_models: int, forecast_horizon: int):\n",
        "        super().__init__()\n",
        "        self.num_models = num_models\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        # The input feature is all L1 models' predictions for a single time step\n",
        "        self.fc1 = nn.Linear(num_models, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x has shape (batch_size, num_l1_models, forecast_horizon)\n",
        "        # We need to process each horizon step independently\n",
        "        # Reshape to (batch_size * forecast_horizon, num_l1_models)\n",
        "        batch_size = x.shape[0]\n",
        "        x_reshaped = x.permute(0, 2, 1).reshape(-1, self.num_models)\n",
        "\n",
        "        h = F.relu(self.fc1(x_reshaped))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        output = self.fc3(h)\n",
        "\n",
        "        # Reshape back to (batch_size, forecast_horizon)\n",
        "        return output.view(batch_size, self.forecast_horizon)\n",
        "\n",
        "# --- L3 Aggregator ---\n",
        "\n",
        "class GreedyEnsemble:\n",
        "    \"\"\"Implements greedy ensemble selection (Caruana et al., 2004).\"\"\"\n",
        "    def __init__(self, num_steps: int = 100):\n",
        "        self.num_steps = num_steps\n",
        "        self.weights = None\n",
        "        self.model_indices = []\n",
        "\n",
        "    def fit(self, x: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"Learns the ensemble weights.\n",
        "\n",
        "        Args:\n",
        "            x: L2 predictions of shape (num_samples, num_l2_models, forecast_horizon).\n",
        "            y: Ground truth of shape (num_samples, forecast_horizon).\n",
        "        \"\"\"\n",
        "        print(f\"Fitting Greedy Ensemble for {self.num_steps} steps...\")\n",
        "        num_samples, num_models, horizon = x.shape\n",
        "        ensemble_preds = np.zeros_like(y)\n",
        "        self.model_indices = []\n",
        "\n",
        "        for step in range(self.num_steps):\n",
        "            best_model_idx = -1\n",
        "            min_error = float('inf')\n",
        "\n",
        "            for i in range(num_models):\n",
        "                # Try adding model i to the current ensemble\n",
        "                temp_ensemble = (ensemble_preds * (step) + x[:, i, :]) / (step + 1)\n",
        "                error = np.mean(np.abs(temp_ensemble - y)) # MAE\n",
        "\n",
        "                if error < min_error:\n",
        "                    min_error = error\n",
        "                    best_model_idx = i\n",
        "\n",
        "            # Add the best model to the ensemble\n",
        "            ensemble_preds = (ensemble_preds * step + x[:, best_model_idx, :]) / (step + 1)\n",
        "            self.model_indices.append(best_model_idx)\n",
        "            if (step + 1) % 20 == 0:\n",
        "                 print(f\"  Step {step+1}/{self.num_steps}, Best Model Added: {best_model_idx}, New Ensemble MAE: {min_error:.4f}\")\n",
        "\n",
        "        # Final weights are the counts of how many times each model was picked\n",
        "        self.weights = np.bincount(self.model_indices, minlength=num_models).astype(float)\n",
        "        self.weights /= self.weights.sum()\n",
        "        print(f\"Final L3 Weights: {self.weights}\")\n",
        "\n",
        "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
        "        # x: L2 predictions of shape (num_samples, num_l2_models, forecast_horizon)\n",
        "        if self.weights is None:\n",
        "            raise RuntimeError(\"Must call fit() before predict().\")\n",
        "        # Apply weights for a weighted average\n",
        "        return np.einsum('m,smh->sh', self.weights, x)\n",
        "\n",
        "# Instantiate models to check parameter counts\n",
        "_l1_linear = LinearModel(CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "_l1_mlp = MLPModel(CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "\n",
        "print(f\"L1 LinearModel params: {sum(p.numel() for p in _l1_linear.parameters())}\")\n",
        "print(f\"L1 MLPModel params: {sum(p.numel() for p in _l1_mlp.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40cea13",
      "metadata": {
        "id": "d40cea13"
      },
      "source": [
        "## 6. Loss Function & Training Utilities\n",
        "\n",
        "For our educational implementation, we'll use a standard Mean Absolute Error (MAE) loss, which is equivalent to the paper's Mean Absolute Scaled Error (MASE) without the scaling factor. This is a robust choice for regression and aligns with the point-forecasting evaluation in the paper (which is SQL at the 0.5 quantile). We'll also create a generic training loop for our PyTorch-based models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bce604",
      "metadata": {
        "id": "b6bce604"
      },
      "outputs": [],
      "source": [
        "def create_sliding_windows(data: np.ndarray, context_length: int, forecast_horizon: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Creates input/output windows from time series data.\"\"\"\n",
        "    X, Y = [], []\n",
        "    num_series, num_steps = data.shape\n",
        "    for i in range(num_series):\n",
        "        for j in range(num_steps - context_length - forecast_horizon + 1):\n",
        "            X.append(data[i, j:j+context_length])\n",
        "            Y.append(data[i, j+context_length:j+context_length+forecast_horizon])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def train_pytorch_model(model: nn.Module,\n",
        "                        data: np.ndarray,\n",
        "                        context_length: int,\n",
        "                        forecast_horizon: int,\n",
        "                        is_stacker: bool = False,\n",
        "                        epochs: int = 20,\n",
        "                        lr: float = 0.001,\n",
        "                        verbose: bool = True):\n",
        "    \"\"\"Generic training loop for a PyTorch model.\"\"\"\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss() # MAE Loss\n",
        "\n",
        "    if not is_stacker:\n",
        "        # L1 model training: data is raw time series\n",
        "        X_train, y_train = create_sliding_windows(data, context_length, forecast_horizon)\n",
        "    else:\n",
        "        # L2 stacker training: data is a tuple of (L1_preds, ground_truth)\n",
        "        X_train, y_train = data\n",
        "\n",
        "    if len(X_train) == 0:\n",
        "        if verbose: print(\"  Warning: No training data available for this fold. Skipping training.\")\n",
        "        return\n",
        "\n",
        "    dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for x_batch, y_batch in loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x_batch)\n",
        "            loss = criterion(output, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f'  Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(loader):.4f}')\n",
        "\n",
        "\n",
        "# --- Prediction utilities ---\n",
        "\n",
        "def predict_pytorch_model(model: nn.Module, x_data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Generic prediction function for a PyTorch model.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x_tensor = torch.from_numpy(x_data).to(device)\n",
        "        preds = model(x_tensor)\n",
        "    return preds.cpu().numpy()\n",
        "\n",
        "# Demonstrate loss computation\n",
        "print(\"Demonstrating loss computation for a LinearStacker:\")\n",
        "num_l1_models_demo = 3\n",
        "stacker_demo = LinearStacker(num_l1_models_demo, FORECAST_HORIZON)\n",
        "\n",
        "# Dummy L1 preds: (batch_size, num_models, horizon)\n",
        "dummy_l1_preds = torch.randn(4, num_l1_models_demo, FORECAST_HORIZON)\n",
        "print(f\"Input L1 preds shape: {dummy_l1_preds.shape}\")\n",
        "\n",
        "# Dummy ground truth: (batch_size, horizon)\n",
        "dummy_y_true = torch.randn(4, FORECAST_HORIZON)\n",
        "print(f\"Ground truth shape: {dummy_y_true.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "stacked_pred = stacker_demo(dummy_l1_preds)\n",
        "print(f\"Output stacked pred shape: {stacked_pred.shape}\")\n",
        "\n",
        "# Loss calculation\n",
        "loss = F.l1_loss(stacked_pred, dummy_y_true)\n",
        "print(f\"Calculated MAE loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7840e78",
      "metadata": {
        "id": "b7840e78"
      },
      "source": [
        "## 7. Baseline Implementations\n",
        "\n",
        "Before diving into the full multi-layer framework, we need to establish our baselines. The paper compares against several, but two of the most important are:\n",
        "\n",
        "1.  **Model Selection**: Instead of ensembling, simply pick the single best-performing L1 model based on its performance on the validation (out-of-fold) data.\n",
        "2.  **Simple Average (Median)**: This is a strong, non-learned ensemble. We use our `MedianStacker` for this. It's a simple but often surprisingly effective baseline.\n",
        "\n",
        "We will evaluate these baselines in the final experiment section. For now, we will focus on implementing the core stacking algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13e24f94",
      "metadata": {
        "id": "13e24f94"
      },
      "source": [
        "## 8. Paper's Main Algorithm — Training the Ensembles\n",
        "\n",
        "This is the core of the implementation, where we build the machinery for single-layer and multi-layer stacking. The key process, as outlined in Figures 1 and 2 of the paper, is the careful generation of out-of-fold predictions to prevent information leakage.\n",
        "\n",
        "### Time Series Cross-Validation\n",
        "\n",
        "We first need a function to generate the training data for the L2 stackers. This involves a K-fold cross-validation scheme that respects the temporal order of the data.\n",
        "\n",
        "For each fold `k` out of `K`:\n",
        "1.  We split the training data. The L1 models are trained on an initial part of the series.\n",
        "2.  They are then used to predict the *next* `H` steps, which were held out.\n",
        "3.  These predictions (called out-of-fold, or OOF, predictions) and their corresponding true values form the training data for the L2 stackers.\n",
        "\n",
        "This ensures that the L2 stacker is trained to correct the errors of L1 models on data they have never seen, which is crucial for generalization.\n",
        "\n",
        "### Single-Layer Stacking\n",
        "\n",
        "Once we have the L1 OOF predictions from all `K` folds, we can train a single L2 stacker model (e.g., `LinearStacker`) on this entire dataset.\n",
        "\n",
        "### Multi-Layer Stacking\n",
        "\n",
        "This extends the process, as described in Section 4.2 of the paper:\n",
        "1.  Generate L1 OOF predictions for all `K` folds.\n",
        "2.  **Split this data**: Use the first `K-1` folds to train the L2 stackers.\n",
        "3.  **Generate L2 OOF predictions**: Use the trained L2 stackers to make predictions on the `K`-th fold of the L1 OOF data. This becomes the training data for the L3 aggregator.\n",
        "4.  **Train L3 Aggregator**: Train our `GreedyEnsemble` on the L2 OOF predictions.\n",
        "5.  **Retrain L2 Models**: Finally, retrain the L2 models on *all* `K` folds of the L1 OOF data so they have access to the most recent patterns before final inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50687c92",
      "metadata": {
        "id": "50687c92"
      },
      "outputs": [],
      "source": [
        "def generate_l1_oof_predictions(l1_models: dict, data: np.ndarray, num_folds: int, context_length: int, forecast_horizon: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implements time series cross-validation to get out-of-fold predictions from L1 models.\n",
        "    This corresponds to Step 1 in Figure 1.B and Figure 2.B.\n",
        "    \"\"\"\n",
        "    print(\"--- Generating L1 Out-of-Fold (OOF) Predictions ---\")\n",
        "\n",
        "    num_series, num_steps = data.shape\n",
        "    # The total length available for training/validation windows\n",
        "    max_train_len = num_steps - forecast_horizon\n",
        "\n",
        "    all_oof_preds = defaultdict(list)\n",
        "    all_oof_targets = []\n",
        "\n",
        "    for k in range(num_folds):\n",
        "        # Step 1a: Define a temporal split point.\n",
        "        # This is the 'L1-Train' part in the paper's diagrams.\n",
        "        # j = (K - k) in the paper's notation (if k starts from 0)\n",
        "        end_idx = max_train_len - (num_folds - 1 - k) * forecast_horizon\n",
        "        train_fold_data = data[:, :end_idx]\n",
        "\n",
        "        # The validation window is the next H steps.\n",
        "        # This is 'L1-Pred' target in the paper's diagrams.\n",
        "        val_context = data[:, end_idx - context_length : end_idx]\n",
        "        val_target = data[:, end_idx : end_idx + forecast_horizon]\n",
        "\n",
        "        print(f\"Fold {k+1}/{num_folds}: Training L1 models on data up to step {end_idx}\")\n",
        "\n",
        "        if val_target.shape[0] == 0: continue\n",
        "\n",
        "        all_oof_targets.append(val_target)\n",
        "\n",
        "        # Step 1b & 1c: Train each base model and predict.\n",
        "        for name, model in l1_models.items():\n",
        "            if isinstance(model, nn.Module):\n",
        "                # Create a fresh instance to avoid continued training\n",
        "                if isinstance(model, LinearModel):\n",
        "                    current_model = LinearModel(context_length, forecast_horizon)\n",
        "                else:\n",
        "                    current_model = MLPModel(context_length, forecast_horizon)\n",
        "                train_pytorch_model(current_model, train_fold_data, context_length, forecast_horizon, verbose=False)\n",
        "                preds = predict_pytorch_model(current_model, val_context)\n",
        "            else: # For scikit-learn style models\n",
        "                model.fit(None, None) # fit is a no-op for Naive\n",
        "                preds = model.predict(val_context)\n",
        "            all_oof_preds[name].append(preds)\n",
        "\n",
        "    # Step 3: Collect all out-of-fold predictions.\n",
        "    # L1-Pred from the paper is the input for the L2 stacker.\n",
        "    oof_targets = np.concatenate(all_oof_targets, axis=0)\n",
        "\n",
        "    # Stack predictions from all models: shape (num_samples, num_models, horizon)\n",
        "    oof_preds_stacked = np.stack([np.concatenate(all_oof_preds[name], axis=0) for name in l1_models.keys()], axis=1)\n",
        "\n",
        "    print(f\"Finished L1 OOF generation.\")\n",
        "    print(f\"Shape of L1 OOF predictions (L1-Pred features): {oof_preds_stacked.shape}\")\n",
        "    print(f\"Shape of L1 OOF targets (L1-Pred targets): {oof_targets.shape}\")\n",
        "\n",
        "    return oof_preds_stacked, oof_targets\n",
        "\n",
        "\n",
        "def train_multi_layer_stacker(l1_models, l2_stackers, l3_aggregator, data, num_folds, context_length, forecast_horizon):\n",
        "    \"\"\"\n",
        "    Main training orchestrator for the full multi-layer stack ensemble.\n",
        "    Follows the procedure from Figure 2.B in the paper.\n",
        "    \"\"\"\n",
        "    # Step 1: Train L1 models with K-fold CV to get L1-Pred\n",
        "    l1_oof_preds, l1_oof_targets = generate_l1_oof_predictions(l1_models, data, num_folds, context_length, forecast_horizon)\n",
        "\n",
        "    # Step 2: Split L1-Pred for L2/L3 training\n",
        "    # The paper uses the last window for the L3 model. The size of one window is num_series.\n",
        "    num_series = data.shape[0]\n",
        "    l2_train_preds = l1_oof_preds[:-num_series, :, :]\n",
        "    l2_train_targets = l1_oof_targets[:-num_series, :]\n",
        "    l3_train_preds_input = l1_oof_preds[-num_series:, :, :]\n",
        "    l3_train_targets = l1_oof_targets[-num_series:, :]\n",
        "\n",
        "    print(\"\\n--- Training L2 Stackers and Generating L3 Training Data ---\")\n",
        "    l2_oof_preds = []\n",
        "    for name, stacker in l2_stackers.items():\n",
        "        print(f\"Training L2 Stacker: {name}\")\n",
        "        # Step 3a: Train L2 stacker on the first K-1 folds of L1-Pred\n",
        "        if isinstance(stacker, nn.Module):\n",
        "            train_pytorch_model(stacker, (l2_train_preds, l2_train_targets), context_length, forecast_horizon, is_stacker=True, epochs=50)\n",
        "            # Step 3b: Generate L2 OOF predictions on the K'th fold\n",
        "            preds = predict_pytorch_model(stacker, l3_train_preds_input)\n",
        "        else:\n",
        "            stacker.fit(l2_train_preds, l2_train_targets)\n",
        "            preds = stacker.predict(l3_train_preds_input)\n",
        "        l2_oof_preds.append(preds)\n",
        "\n",
        "    # This is L2-Pred: input for the L3 model\n",
        "    l2_oof_preds_stacked = np.stack(l2_oof_preds, axis=1)\n",
        "    print(f\"Shape of L2 OOF predictions (L3 training features): {l2_oof_preds_stacked.shape}\")\n",
        "    print(f\"Shape of L3 training targets: {l3_train_targets.shape}\")\n",
        "\n",
        "    # Step 4: Train L3 model\n",
        "    print(\"\\n--- Training L3 Aggregator ---\")\n",
        "    l3_aggregator.fit(l2_oof_preds_stacked, l3_train_targets)\n",
        "\n",
        "    # Step 5 & 6: Retrain L1 and L2 models on all available data for inference\n",
        "    print(\"\\n--- Retraining Models on Full Data ---\")\n",
        "    print(\"Retraining L1 models...\")\n",
        "    for name, model in l1_models.items():\n",
        "        if isinstance(model, nn.Module):\n",
        "            train_pytorch_model(model, data, context_length, forecast_horizon, verbose=False, epochs=50)\n",
        "\n",
        "    print(\"Retraining L2 stackers...\")\n",
        "    for name, stacker in l2_stackers.items():\n",
        "        if isinstance(stacker, nn.Module):\n",
        "            train_pytorch_model(stacker, (l1_oof_preds, l1_oof_targets), context_length, forecast_horizon, is_stacker=True, epochs=50, verbose=False)\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "    return l1_models, l2_stackers, l3_aggregator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "127c0b89",
      "metadata": {
        "id": "127c0b89"
      },
      "source": [
        "## 9. Inference / Generation\n",
        "\n",
        "With the fully trained multi-layer ensemble, making a final prediction is a three-step sequential process:\n",
        "\n",
        "1.  **L1 Prediction**: The input time series history is fed into all the retrained L1 base models to generate a set of initial forecasts.\n",
        "2.  **L2 Prediction**: The collected L1 predictions are then passed as features to all the retrained L2 stacker models, which produce a new, refined set of forecasts.\n",
        "3.  **L3 Aggregation**: Finally, the L2 predictions are given to the trained L3 aggregator, which computes the final weighted average to produce the ultimate forecast.\n",
        "\n",
        "We will create a function that encapsulates this entire pipeline and demonstrates it on a sample from the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a2b0119",
      "metadata": {
        "id": "4a2b0119"
      },
      "outputs": [],
      "source": [
        "def multilayer_inference(input_history: np.ndarray, l1_models: dict, l2_stackers: dict, l3_aggregator) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Performs the full 3-step inference process.\n",
        "\n",
        "    Args:\n",
        "        input_history: A batch of time series contexts, shape (batch_size, context_length).\n",
        "    \"\"\"\n",
        "    print(\"--- Performing Multi-layer Inference ---\")\n",
        "\n",
        "    # Step 1: L1 Prediction\n",
        "    l1_preds = []\n",
        "    for name, model in l1_models.items():\n",
        "        if isinstance(model, nn.Module):\n",
        "            pred = predict_pytorch_model(model, input_history)\n",
        "        else:\n",
        "            pred = model.predict(input_history)\n",
        "        l1_preds.append(pred)\n",
        "    l1_preds_stacked = np.stack(l1_preds, axis=1)\n",
        "    print(f\"L1 predictions shape: {l1_preds_stacked.shape}\")\n",
        "\n",
        "    # Step 2: L2 Prediction\n",
        "    l2_preds = []\n",
        "    for name, stacker in l2_stackers.items():\n",
        "        if isinstance(stacker, nn.Module):\n",
        "            pred = predict_pytorch_model(stacker, l1_preds_stacked)\n",
        "        else:\n",
        "            pred = stacker.predict(l1_preds_stacked)\n",
        "        l2_preds.append(pred)\n",
        "    l2_preds_stacked = np.stack(l2_preds, axis=1)\n",
        "    print(f\"L2 predictions shape: {l2_preds_stacked.shape}\")\n",
        "\n",
        "    # Step 3: L3 Aggregation\n",
        "    final_forecast = l3_aggregator.predict(l2_preds_stacked)\n",
        "    print(f\"Final forecast shape: {final_forecast.shape}\")\n",
        "\n",
        "    return final_forecast\n",
        "\n",
        "\n",
        "# --- Instantiate all models for the experiment ---\n",
        "l1_models_to_train = {\n",
        "    'SeasonalNaive': SeasonalNaiveModel(seasonality=24),\n",
        "    'Linear': LinearModel(CONTEXT_LENGTH, FORECAST_HORIZON),\n",
        "    'MLP': MLPModel(CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "}\n",
        "\n",
        "l2_stackers_to_train = {\n",
        "    'Median': MedianStacker(),\n",
        "    'LinearStacker': LinearStacker(len(l1_models_to_train), FORECAST_HORIZON),\n",
        "    'MLPStacker': MLPStacker(len(l1_models_to_train), FORECAST_HORIZON)\n",
        "}\n",
        "\n",
        "l3_aggregator_to_train = GreedyEnsemble(num_steps=100)\n",
        "\n",
        "# Run the full training pipeline\n",
        "l1_models_final, l2_stackers_final, l3_aggregator_final = train_multi_layer_stacker(\n",
        "    l1_models_to_train,\n",
        "    l2_stackers_to_train,\n",
        "    l3_aggregator_to_train,\n",
        "    train_data,\n",
        "    NUM_FOLDS,\n",
        "    CONTEXT_LENGTH,\n",
        "    FORECAST_HORIZON\n",
        ")\n",
        "\n",
        "# Demonstrate inference on one test sample\n",
        "sample_context = test_data[0:1, -CONTEXT_LENGTH-FORECAST_HORIZON:-FORECAST_HORIZON]\n",
        "print(f\"\\nSample context shape: {sample_context.shape}\")\n",
        "final_prediction = multilayer_inference(sample_context, l1_models_final, l2_stackers_final, l3_aggregator_final)\n",
        "print(f\"Final prediction for one sample: \\n{final_prediction[0, :5]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df109e3b",
      "metadata": {
        "id": "df109e3b"
      },
      "source": [
        "## 10. Full Experiment & Evaluation\n",
        "\n",
        "Now we will systematically evaluate the performance of our trained multi-layer stacker against key baselines on the held-out test set. We will compare four approaches:\n",
        "\n",
        "1.  **Best L1 Model (Model Selection)**: We find the single best L1 model based on its OOF performance and use it for all test predictions.\n",
        "2.  **Median Baseline**: A simple but strong ensemble that takes the median of all L1 model predictions.\n",
        "3.  **Single-Layer Stacker (Linear)**: A learned ensemble using our trained `LinearStacker` to combine L1 predictions. This represents a single-layer learned approach.\n",
        "4.  **Multi-Layer Stacker**: The full, final model from our pipeline.\n",
        "\n",
        "We will compute the Mean Absolute Error (MAE) for each method across all time series and all forecasted time steps in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efb15f8",
      "metadata": {
        "id": "9efb15f8"
      },
      "outputs": [],
      "source": [
        "def evaluate_performance():\n",
        "    \"\"\"Runs the full evaluation on the test set and prints a results table.\"\"\"\n",
        "    print(\"\\n--- Running Final Evaluation on Test Set ---\")\n",
        "\n",
        "    # Prepare test data\n",
        "    X_test, y_test = create_sliding_windows(test_data, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "\n",
        "    # --- Method 1: Best L1 Model (Model Selection) ---\n",
        "    # Find best L1 model based on OOF performance\n",
        "    l1_oof_preds, l1_oof_targets = generate_l1_oof_predictions(l1_models_to_train, train_data, NUM_FOLDS, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "    l1_model_names = list(l1_models_to_train.keys())\n",
        "    l1_oof_errors = []\n",
        "    for i in range(len(l1_model_names)):\n",
        "        error = np.mean(np.abs(l1_oof_preds[:, i, :] - l1_oof_targets))\n",
        "        l1_oof_errors.append(error)\n",
        "        print(f\"L1 Model '{l1_model_names[i]}' OOF MAE: {error:.4f}\")\n",
        "\n",
        "    best_l1_idx = np.argmin(l1_oof_errors)\n",
        "    best_l1_name = l1_model_names[best_l1_idx]\n",
        "    best_l1_model = l1_models_final[best_l1_name]\n",
        "    print(f\"==> Best L1 Model selected: {best_l1_name}\")\n",
        "\n",
        "    if isinstance(best_l1_model, nn.Module):\n",
        "        best_l1_preds = predict_pytorch_model(best_l1_model, X_test)\n",
        "    else:\n",
        "        best_l1_preds = best_l1_model.predict(X_test)\n",
        "    mae_best_l1 = np.mean(np.abs(best_l1_preds - y_test))\n",
        "\n",
        "    # --- Generate L1 predictions on test set for other methods ---\n",
        "    l1_test_preds = []\n",
        "    for name, model in l1_models_final.items():\n",
        "        if isinstance(model, nn.Module):\n",
        "            pred = predict_pytorch_model(model, X_test)\n",
        "        else:\n",
        "            pred = model.predict(X_test)\n",
        "        l1_test_preds.append(pred)\n",
        "    l1_test_preds_stacked = np.stack(l1_test_preds, axis=1)\n",
        "\n",
        "    # --- Method 2: Median Baseline ---\n",
        "    median_stacker = MedianStacker()\n",
        "    median_preds = median_stacker.predict(l1_test_preds_stacked)\n",
        "    mae_median = np.mean(np.abs(median_preds - y_test))\n",
        "\n",
        "    # --- Method 3: Single-Layer Stacker (Linear) ---\n",
        "    single_layer_linear_stacker = l2_stackers_final['LinearStacker']\n",
        "    single_layer_preds = predict_pytorch_model(single_layer_linear_stacker, l1_test_preds_stacked)\n",
        "    mae_single_layer = np.mean(np.abs(single_layer_preds - y_test))\n",
        "\n",
        "    # --- Method 4: Multi-Layer Stacker ---\n",
        "    multi_layer_preds = multilayer_inference(X_test, l1_models_final, l2_stackers_final, l3_aggregator_final)\n",
        "    mae_multi_layer = np.mean(np.abs(multi_layer_preds - y_test))\n",
        "\n",
        "    # --- Print Results Table ---\n",
        "    print(\"\\n--- Final Performance Results (MAE on Test Set) ---\")\n",
        "    print(\"=====================================================\")\n",
        "    print(f\"| Method                    | MAE     | Improvement vs Median |\")\n",
        "    print(\"|---------------------------|---------|-----------------------|\")\n",
        "    print(f\"| Best L1 ({best_l1_name})       | {mae_best_l1:.4f} | {100*(mae_median-mae_best_l1)/mae_median:6.2f}%               |\")\n",
        "    print(f\"| Median Baseline           | {mae_median:.4f} | 0.00%                 |\")\n",
        "    print(f\"| Single-Layer (Linear)     | {mae_single_layer:.4f} | {100*(mae_median-mae_single_layer)/mae_median:6.2f}%               |\")\n",
        "    print(f\"| Multi-Layer Stacker       | {mae_multi_layer:.4f} | {100*(mae_median-mae_multi_layer)/mae_median:6.2f}%               |\")\n",
        "    print(\"=====================================================\")\n",
        "\n",
        "    return {\n",
        "        \"Best L1\": best_l1_preds,\n",
        "        \"Median\": median_preds,\n",
        "        \"Multi-Layer\": multi_layer_preds,\n",
        "        \"Ground Truth\": y_test\n",
        "    }\n",
        "\n",
        "all_predictions = evaluate_performance()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a5fc7ec",
      "metadata": {
        "id": "5a5fc7ec"
      },
      "source": [
        "## 11. Visualizations\n",
        "\n",
        "Quantitative metrics are essential, but visualizations provide deeper insight into model behavior. We'll create several plots to analyze our results:\n",
        "\n",
        "1.  **Forecast Comparison on a Test Series**: We'll plot the ground truth against the forecasts from our best baseline (Median) and our final Multi-Layer Stacker for a single test series. This lets us visually inspect where the sophisticated model improves upon the simpler one.\n",
        "2.  **L3 Aggregator Weights**: A bar chart showing the weights the L3 `GreedyEnsemble` assigned to each L2 stacker. This is analogous to Figure 3 in the paper and reveals which combination strategies the model found most useful.\n",
        "3.  **Overall Performance Comparison**: A bar chart summarizing the final MAE results from our experiment, providing a clear comparison of all evaluated methods.\n",
        "4.  **L2 Stacker Training Data**: A scatter plot showing the relationship between L1 model predictions and the ground truth on the out-of-fold data. This visualizes the input space for the L2 models and helps us understand the errors they are learning to correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eebfd27f",
      "metadata": {
        "id": "eebfd27f"
      },
      "outputs": [],
      "source": [
        "# Plot 1: Forecast Comparison on a Test Series\n",
        "\n",
        "def plot_forecast_comparison(predictions: dict, series_idx: int):\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.title(f\"Forecast Comparison on Test Series #{series_idx}\")\n",
        "\n",
        "    # Get a continuous segment for plotting\n",
        "    context = test_data[series_idx, -CONTEXT_LENGTH-FORECAST_HORIZON:-FORECAST_HORIZON]\n",
        "    full_truth = test_data[series_idx, -CONTEXT_LENGTH-FORECAST_HORIZON:]\n",
        "\n",
        "    time_hist = np.arange(CONTEXT_LENGTH)\n",
        "    time_future = np.arange(CONTEXT_LENGTH, CONTEXT_LENGTH + FORECAST_HORIZON)\n",
        "\n",
        "    # Find the corresponding prediction in the batched results\n",
        "    # This is a simplification; we'll find a window in the test set\n",
        "    # For this visualization, let's just use the first window of the test set\n",
        "    pred_idx = 0\n",
        "    while pred_idx * (NUM_STEPS - CONTEXT_LENGTH - FORECAST_HORIZON + 1) < series_idx:\n",
        "        pred_idx +=1\n",
        "\n",
        "    plt.plot(time_hist, context, 'k-', label='History')\n",
        "    plt.plot(time_future, predictions[\"Ground Truth\"][pred_idx], 'g-', label='Ground Truth', linewidth=2)\n",
        "    plt.plot(time_future, predictions[\"Median\"][pred_idx], 'b--', label='Median Baseline')\n",
        "    plt.plot(time_future, predictions[\"Multi-Layer\"][pred_idx], 'r-', label='Multi-Layer Stack')\n",
        "\n",
        "    plt.axvline(x=CONTEXT_LENGTH, color='gray', linestyle='--', label='Forecast Start')\n",
        "    plt.xlabel(\"Time Step\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_forecast_comparison(all_predictions, series_idx=5)\n",
        "\n",
        "# Plot 2: L3 Aggregator Weights\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"L3 Aggregator: Weights Assigned to L2 Stackers\", fontsize=14)\n",
        "l2_names = list(l2_stackers_final.keys())\n",
        "weights = l3_aggregator_final.weights\n",
        "plt.bar(l2_names, weights, color=['skyblue', 'salmon', 'lightgreen'])\n",
        "plt.ylabel(\"Weight in Ensemble\", fontsize=12)\n",
        "plt.xlabel(\"L2 Stacker Model\", fontsize=12)\n",
        "for i, w in enumerate(weights):\n",
        "    plt.text(i, w + 0.01, f'{w:.2f}', ha='center', fontsize=12)\n",
        "plt.ylim(0, max(weights) * 1.2)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Overall Performance Comparison\n",
        "results_mae = {\n",
        "    f\"Best L1\": np.mean(np.abs(all_predictions[\"Best L1\"] - all_predictions[\"Ground Truth\"])),\n",
        "    \"Median\": np.mean(np.abs(all_predictions[\"Median\"] - all_predictions[\"Ground Truth\"])),\n",
        "    \"Multi-Layer\": np.mean(np.abs(all_predictions[\"Multi-Layer\"] - all_predictions[\"Ground Truth\"]))\n",
        "}\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Overall Performance Comparison (MAE on Test Set)\", fontsize=14)\n",
        "methods = list(results_mae.keys())\n",
        "mae_values = list(results_mae.values())\n",
        "bars = plt.bar(methods, mae_values, color=['lightcoral', 'cornflowerblue', 'seagreen'])\n",
        "plt.ylabel(\"Mean Absolute Error (MAE)\", fontsize=12)\n",
        "plt.xlabel(\"Method\", fontsize=12)\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.4f}', ha='center', va='bottom')\n",
        "plt.ylim(0, max(mae_values) * 1.15)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: L2 Stacker Training Data Visualization\n",
        "l1_oof_preds, l1_oof_targets = generate_l1_oof_predictions(l1_models_to_train, train_data, NUM_FOLDS, CONTEXT_LENGTH, FORECAST_HORIZON)\n",
        "# Flatten for plotting: we'll plot each time step as a point\n",
        "flat_targets = l1_oof_targets.flatten()\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.suptitle(\"L2 Stacker Training Data (L1 OOF Predictions vs. Ground Truth)\", fontsize=16)\n",
        "for i, name in enumerate(l1_models_to_train.keys()):\n",
        "    ax = plt.subplot(1, 3, i + 1)\n",
        "    flat_preds = l1_oof_preds[:, i, :].flatten()\n",
        "    ax.scatter(flat_targets, flat_preds, alpha=0.1, label=f'{name} Predictions')\n",
        "    ax.plot([-20, 20], [-20, 20], 'r--', label='Perfect Forecast') # Identity line\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(\"Ground Truth\")\n",
        "    ax.set_ylabel(\"L1 Prediction\")\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    ax.set_xlim(min(flat_targets), max(flat_targets))\n",
        "    ax.set_ylim(min(flat_targets), max(flat_targets))\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc7bb35",
      "metadata": {
        "id": "bdc7bb35"
      },
      "source": [
        "## 12. Summary & Next Steps\n",
        "\n",
        "### Observations\n",
        "\n",
        "In this notebook, we successfully implemented the multi-layer stacking framework for time series forecasting. Our experiments on a synthetic dataset yielded results that are qualitatively consistent with the paper's main findings:\n",
        "\n",
        "-   **Stacking Outperforms Baselines**: Our Multi-Layer Stacker achieved the lowest Mean Absolute Error on the test set, demonstrating a clear improvement over both the Median baseline and selecting the single best L1 model.\n",
        "-   **Learned Ensembles Add Value**: The single-layer `LinearStacker` also outperformed the baselines, showing that even a simple learned combination is more effective than a naive average or model selection.\n",
        "-   **The L3 Aggregator Learns a Diverse Portfolio**: The L3 `GreedyEnsemble` assigned non-zero weights to multiple L2 stackers, including both simple (Median) and complex (MLPStacker) models. This confirms the paper's core hypothesis: since no single L2 stacker is universally best, learning to combine them adaptively leads to more robust performance.\n",
        "\n",
        "### Scaling to the Paper's Level\n",
        "\n",
        "Our implementation was a scaled-down, educational version. To replicate the paper's results at full scale, several changes would be necessary:\n",
        "\n",
        "-   **More Sophisticated L1/L2 Models**: We used simple Linear and MLP models. A full implementation would incorporate the wide range of models used in the paper, such as Transformers (PatchTST), RNNs (DeepAR), GBTs (LightGBM), and statistical methods (AutoETS, Theta).\n",
        "-   **Probabilistic Forecasting**: We focused on point forecasting using MAE loss. The paper's primary metric is Scaled Quantile Loss (SQL) for probabilistic forecasts. This would require modifying the models to output multiple quantiles and using a pinball loss function for training.\n",
        "-   **Larger, Real-World Datasets**: The paper evaluates on 50 real-world datasets. Running the framework on such a scale would require significant computational resources and more robust data loading and preprocessing pipelines.\n",
        "-   **More L2 Stacker Variants**: We only implemented three L2 stackers. The paper evaluates 31 variants, including many types of linear models with different weight-tying and constraint schemes.\n",
        "\n",
        "### Ideas for Extension\n",
        "\n",
        "This implementation provides a strong foundation for further exploration:\n",
        "\n",
        "-   **Implement More L2 Stackers**: Add more of the linear model variants from the paper (Section 3.1) to see how they contribute to the L3 ensemble.\n",
        "-   **Explore Different L3 Aggregators**: Replace the `GreedyEnsemble` with a different model, such as a simple `LinearStacker` or even a learned model selection mechanism.\n",
        "-   **Dynamic L1 Model Selection**: Before stacking, one could implement a pruning strategy to remove consistently poor-performing L1 models, potentially speeding up training and inference without hurting accuracy.\n",
        "-   **Meta-Learning for Weights**: Instead of learning weights per dataset, one could explore meta-learning approaches like FFORMA to predict ensemble weights based on time series features, which could lead to better generalization for new, unseen datasets."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}